{
  "id": "e493bf5d-6bcc-4eb4-a0b3-86c5475a4a62",
  "source_type": "paper",
  "source_url": null,
  "source_file_path": "/Users/dpickem/workspace/project_second_brain/scripts/test_data/sample_paper_wild_ma.pdf",
  "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
  "authors": [
    "Ri-Zhao Qiu",
    "Yuchen Song",
    "Xuanbin Peng",
    "Sai Aneesh Suryadevara",
    "Ge Yang",
    "Minghuan Liu",
    "Mazeyu Ji",
    "Chengzhe Jia",
    "Ruihan Yang",
    "Xueyan Zou",
    "Xiaolong Wang"
  ],
  "created_at": "2026-01-01T20:20:31.210335",
  "ingested_at": "2026-01-01T20:20:31.210354",
  "full_text": "# WildMa\n\n# WildLMa: Long Horizon Loco-Manipulation in the Wild\n\nRi-Zhao Qiu\\*1, Yuchen Song\\*1, Xuanbin Peng\\*1, Sai Aneesh Suryadevara1, Ge Yang2, Minghuan Liu1\n\nMazeyu Ji $^{1}$ , Chengzhe Jia $^{1}$ , Ruihan Yang $^{1}$ , Xueyan Zou $^{1}$ , Xiaolong Wang $^{1,3}$\n\n*equal contribution\n\n$^{1}$ UC San Diego  $^{2}$ MIT  $^{3}$ NVIDIA\n\nhttps://wildlma.github.io\n\n![img-0.jpeg](img-0.jpeg)\n(a) In-The-Wild Quadruped Mobile Manipulation\n\n![img-1.jpeg](img-1.jpeg)\n(b) Whole-body VR Teleoperation\n(c) Skill Learning\nFig. 1: WildLMa implements a framework for in-the-wild manipulation with a quadruped robot, which combines a whole-body controller and imitation learning for effective single-skill learning. (a) Long Horizon Loco-Manipulation in indoor as well as outdoor settings. (b) Teleoperation demonstration for collecting training data for imitation learning. (c) The constructed skill library with various skills, which can be composed by LLM planner to complete complex tasks.\n\nAbstract\u2014'In-the-wild' mobile manipulation aims to deploy robots in diverse real-world environments, which requires the robot to (1) have skills that generalize across object configurations; (2) be capable of long-horizon task execution in diverse environments; and (3) perform complex manipulation beyond pick-and-place. Quadruped robots with manipulators hold promise for extending the workspace and enabling robust locomotion, but existing results do not investigate such a capability. This paper proposes WildLMa with three components to address these issues: (1) adaptation of learned low-level controller for VR-enabled whole-body teleoperation and traversability; (2) WildLMa-Skill \u2014 a library of generalizable visuomotor skills acquired via imitation learning or heuristics and (3) WildLMa-Planner \u2014 an interface of learned skills that allow LLM planners to coordinate skills for long-horizon tasks. We demonstrate the importance of high-quality training data by achieving higher grasping success rate over existing RL baselines using only tens of demonstrations. WildLMa exploits CLIP for language-conditioned imitation learning that empirically generalizes to objects unseen in training demonstrations. Besides extensive quantitative evaluation, we qualitatively demonstrate practical robot applications, such as cleaning up trash in university hallways or outdoor terrains, operating articulated objects, and rearranging items on a bookshelf.\n\n# I. INTRODUCTION\n\nPractical robot mobile manipulation requires generalizable skills and long-horizon task execution. Consider a scenario where a mobile robot is deployed out-of-box at a family house. The robot is tasked with daily chores including collecting the trash around the house and grabbing something\n\nfor human. To accomplish these tasks, the robot needs skills that generalize to unseen objects and a planner capable of compositing skills over a long horizon.\n\nExisting methods [17, 20, 31, 32, 44, 61, 71] have approached mobile manipulation from two primary directions. Modular methods [32, 44, 71] aim at designing decoupled perception-planning modules. With advances in large-scale vision models [28, 34, 45], recent modular methods [32, 44] exhibit strong generalizability in perception to an open set of language-specified objects. However, their planning modules [6, 20, 32, 44] often rely on heuristic-based motion planning, limiting tasks to mostly simple pick-and-place. End-to-end approaches [11, 12, 17, 22, 31, 69], on the other hand, use learned policies to enable robot with complex actions. They, however, often hold a strong assumption of the small training-testing distribution gap (e.g., sim2real [31] or intra-class variation [17]) and thus do not show strong generalizability. In addition, policies learned via imitation learning are prone to compounding error [26, 69] over long-horizon execution. Thus, these learned skills should be designed to be as atomic as possible for both generalizability and accuracy.\n\nThis paper investigates in-the-wild mobile manipulation that addresses these issues for real-world deployment. Specifically, in-the-wild manipulation requires the robot to have skills that (1) generalize across texture, lighting, and diverse environments; (2) are capable of long-horizon execution; and\n\n(3) perform complex manipulation beyond pick-and-place.\n\nTo this end, we propose WildLMa. For generalizability, WildLMa enables language-conditioned imitation learning (WildLMa-Skill). Building upon ACT *[17, 69]*, WildLMa-Skill improves generalizability via pre-trained CLIP and composable skills. Instead of simply using CLIP features *[12, 22]*, we apply a reparameterization trick *[73]* to CLIP to compute probability maps given object text query as an auxiliary input. We then use VR teleoperation *[9, 13]* to collect human demonstrations to acquire complex skills such as non-prehensile manipulation. We adapt a learned low-level controller *[31]* for VR-based whole-body teleoperation, which significantly increases the robot workspace and reduces the demonstration cost by 26.9% compared to the decoupled strategy. Finally, based on a library of acquired generalizable and atomic skills, WildLMa provides a language interface (WildLMa-Planner) that allows interfacing with LLMs to composite skills for long-horizon execution.\n\nIn summary, our contributions are:\n\n- A generic framework with techniques that allow generalizable language-condition imitation learning (WildLMa-Skill) with interfacing to the LLM planner (WildLMa-Planner).\n- Demonstrations of in-the-wild mobile manipulation tasks with full-stack and systematic deployment of the proposed framework.\n- Comprehensive evaluation and ablation for the proposed technique, which paves the way for future study.\n\n## II Related Work\n\n#### Mobile Manipulation\n\nMobile manipulation has gained increasing attention for its vision of enabling robots to perform diverse practical tasks. In terms of hardware configurations, wheeled robots have made substantial strides *[1, 29, 32, 55, 58, 61, 71]* for its reliable base movement *[54]*, while recently, legged robots have also gained more interest for its robust locomotion *[10, 63]* and the extended workspace via whole-body arm-base coordination *[15, 22, 31, 42]*.\n\nCategorized by methodology, existing methods can be divided into modular methods and end-to-end methods. Recent modular approaches *[2, 27, 32, 33, 44, 66, 67, 68, 71]* design decoupled perception-planning strategy. In particular, perception *[32, 44, 71]* are often done by applications of vision foundation models *[21, 28, 34, 45]*; whereas grasping are done by off-the-shelf pose prediction models (e.g., GraspNet *[14]*) and IK solver *[47]*. Despite strong perception designs, modular methods are mostly limited to simple pick-and-place tasks. On the other hand, end-to-end approaches use Reinforcement Learning (RL) *[19, 31, 40, 60, 61]* or Imitation Learning (IL) *[17, 22, 48]* to enable complex tasks beyond pick-and-place such as articulated manipulation *[4, 61]* and non-prehensile manipulation *[17, 22]*. However, these work often fall short when training-testing distributions mismatch.\n\nMost closely related to our work, Yokoyama et al. *[66]* proposed to use sim2real RL for in-the-wild mobile manipulation. However, they do not investigate manipulation tasks other than simple pick-and-place. WildLMa uses imitation learning to learn diverse skills with generalizability, task complexity, and long-horizon run for in-the-wild execution.\n\n#### Long-horizon Mobile Manipulation\n\nFor robots to assist with real-world tasks such as cleaning up home, they need to be capable of dealing with long-horizon mobile manipulation, where independent skills are planned and triggered to complete given goals. Existing methods rely on sampling-based planning *[18, 53]*, RL *[19, 30, 65, 66]*, and Large Language Models (LLMs) *[20, 24, 46, 51]* to coordinate skill primitives for long-horizon task execution. Recent work *[20, 24, 46, 51]* have found that LLM-based methods, especially Large-Mutlimodal Models (LMMs) *[8, 37]*, are promising to serve as effective planners for embodied agents, where the research efforts are centered around hierarchical search *[46]* and re-planning *[71]*. WildLMa is intended to be orthogonal to these existing work in LLM planner. Instead of studying the planning capability, we investigate the potential of interfacing LMMs with skills acquired via imitation learning for practical applications.\n\n#### Imitation Learning\n\nImitation learning has demonstrated promising results through learning from real-world expert demonstrations *[9, 11, 13, 17, 22, 48, 57, 64]*. Investigated for decades since the 80s *[41]*, behavior cloning *[5, 41]* is one of the most commonly used imitation learning approach that learns an end-to-end mapping from observations to actions. Recently, researchers have shown that this classic approach not only allows complex manipulations *[9, 13, 17, 70]*, but also holds the potential that scaling up training data with low-cost hardware *[12, 17, 22, 48, 59, 64, 69]* will lead to generalizable policies. Similar to existing work *[9, 13, 23, 43, 49]*, we also use VR devices to collect expert demonstrations that minimize the expert-agent observation gap *[69]*. To reduce the cross-embodiment gap between the human operator and the quadruped robot, we combine the VR demonstration with learned whole-body controller. In addition, we also improve the vanilla ACT model *[17, 69]* to support language-conditioned imitation learning that is more generalizable with autonomous termination.\n\n#### Whole-body Control\n\nQuadruped Whole-Body Control (WBC) draws inspiration from the natural motions of animals to extend the robot workspace via arm-base coordination. The WBC capability is usually achieved via model-based hierarchical trajectory optimization *[3, 35, 50, 72, 74]* or sim2real RL *[15, 22, 31]*. Our work is based on the low-level controller proposed by VBC *[31]*, which designed a bi-level RL paradigm with a low-level whole-body controller. Notably, some existing work has also attempted to combine teleoperation with whole-body control for quadruped robots *[42, 72]* but does not investigate learning skills from teleoperation. Most related to our work, Ha et al. *[22]* demonstrated whole-body imitation learning with handheld data collection hardware. The main differences between our work and Ha et al. *[22]* are (1) the data collected without teleoperating the robot can include only wrist camera observation, which may lead to worse performance than multi-camera setup as we empirically verify and (2) Ha et al. *[22]*\n\n![img-2.jpeg](img-2.jpeg)\nFig. 2: Overview of WildLMa models and robot setups. (a) WildLMa takes a frozen CLIP model to encode task-specific texts and visual observations; (b) Our robot platform is a Unitree B1 quadruped combined with a Unitree Z1 arm and a 3D-printed gripper, with two RGBD cameras and one lidar mounted on.\n\nfocus on execution of short tasks; while we investigate in-the-wild mobile manipulation with long horizon task execution.\n\n## III. METHOD\n\nWildLMa designs three components to address challenges for in-the-wild mobile manipulation. Sec. III-A describes adapting a whole-body controller to support efficient teleoperation and more diverse real-world tasks. In Sec. III-B, we propose WildLMa-Skill, which modifies the pre-trained CLIP model [45] for generalizable imitation learning. WildLMa-Skill then constructs a skill library consisting of learned skills and analytical skills (e.g., navigation). Finally, WildLMa-Planner (Sec. III-C) interfaces WildLMa-Skill with an LLM planner to carry out long-horizon execution.\n\n## A. Whole-body VR Teleoperation\n\nRecent imitation learning methods have benefited from improved data collection methods via VR/AR-based teleoperation [9, 13, 23, 43]. However, though human operators can naturally tele-operate bipedal humanoid robots [9, 16, 23], it is non-trivial to tele-operate quadruped robots due to the embodiment gap [39] between two-legged human and quadruped robots inspired by four-legged animals.\n\nTo reduce the need for the tele-operator to consider both the base movement and the arm movement, we propose to use a whole-body controller [31] that allows smooth arm-base coordination for the robot. In particular, we use the low-level whole-body policy developed by Liu et al. [31]. Trained with RL, the learned whole-body controller takes in base commands (linear velocity and angular velocity) and 6DOF end effector pose w.r.t. the arm base. The policy outputs arm and base joint commands for coordinated movement that extends the workspace (illustrated in Fig. 1).\n\nBased on the pre-trained low-level controller, we then design an interface for human users to teleoperate the robot. We use the OpenTV framework [9] with Apple Vision Pro, which allows real-time video streaming, tracking of 6DOF poses of head and hands, and 3D gesture keypoints. To\n\nminimize the expert-agent observation gap [69], the teleoperator gets real-time streams of the robot's head camera views and wrist camera views.\n\nTo translate tele-operator movement to robot movement, we linearly transform the operator's right wrist pose (relative to their initial hand pose) $T_{right} \\in \\mathrm{SE}(3)$ into the relative end effector pose $T_{ee} \\in \\mathrm{SE}(3)$. We scale the translations with a constant $s_c$, as we find that the workspace of the Z1 arm is slightly larger than average human arms. More concretely, let $\\mathbf{R}_{right}$ be the rotational component and $\\mathbf{t}_{right}$ be the translational component of $T_{right}$, $T_{ee}$ is given by,\n\n$$\nT _ {e e} = \\left[ \\begin{array}{c c} \\mathbf {R} _ {\\text {r i g h t}} &amp; s _ {c} \\cdot \\mathbf {t} _ {\\text {r i g h t}} \\\\ \\mathbf {0} ^ {\\intercal} &amp; 1 \\end{array} \\right]. \\tag {1}\n$$\n\nThe gripper open-close actions are then naturally mapped from the pinching of the thumb and the index finger (via 3D keypoints). The whole-body controller automatically controls the base rotation to coordinate with the arm. In turn, the tele-operator's left wrist governs planar base movements (e.g., angular and linear velocities). When the tele-operator pinches their left fingers, VR tracks the pose $T_{left}$ as a virtual joystick with deadzone ($x_{th} = 5cm$). We find this simple base command mapping sufficient for the tasks involved.\n\n## B. WildLMa-Skill\n\nWildLMa-Skill contains skills from two categories: skills acquired via imitation learning and with analytical planners.\n\na) WildLMa-Skill - Imitation Learning: The collected real-world demonstrations can be turned into autonomous skills with existing behavior cloning methods [9, 17]. Many existing methods, however, struggle to generalize to novel environments [17]. To improve the generalizability of learned skills without expensive demonstration collection cost, we propose to adapt pre-trained CLIP [45] to ACT [69] for imitation learning of individual skills.\n\nImproving Generalizability with CLIP: We encode camera observations with a frozen CLIP visual backbone. Instead of simply using intermediate CLIP features as in [12, 22], we apply MaskCLIP [73], a reparameterization trick to generate image-text cross attention map. More concretely, let $\\Omega$ be\n\n![img-3.jpeg](img-3.jpeg)\nFig. 3: Overview of WildLMa-planner. Given a constructed hierarchical scene graph, WildLMa-planner adopts a coarse-to-fine searching mechanism to determine node traversal and structured actions to take.\n\n![img-4.jpeg](img-4.jpeg)\n\nthe space of RGB images. The original CLIP [45] is a mapping function  $f_{visual}(\\cdot): \\Omega \\mapsto \\mathbb{R}^C$ , where  $\\mathbb{R}^C$  is the image-text embedding space learned from contrastive learning [45]. MaskCLIP modifies the network architecture to a new mapping function  $g_{visual}(\\cdot): \\Omega \\mapsto \\mathbf{H} \\times \\mathbf{W} \\times \\mathbb{R}^C$ , which is a feature map aligned to the CLIP embedding space  $\\mathbb{R}^C$  (illustrated in Fig. 2).\n\nImage-text Cross Attention. Consistent with findings by Chi et al. [12], we found that the adaptation of CLIP [45] improves the performance. However, when tested with objects unseen in the training demonstrations, the success rate is still unsatisfactory. Thus, we propose to make the acquired skills language-conditioned by introducing cross-attention. We provide task-specific texts during both the training and testing time (e.g., for the ADA door button-pressing task, we use 'door' and 'ADA button') with CLIP text embedding  $f_{text}(\\cdot):Text\\mapsto \\mathbb{R}^C$ . With slight abuse of notation, the text vector can then be compared with the CLIP feature map via cosine similarity\n\n$$\n\\operatorname {C R O S S A T T} (\\cdot , \\cdot) = \\frac {g _ {\\text {v i s u a l}} (\\cdot) f _ {\\text {t e x t}} (\\cdot)}{\\left| \\left| g _ {\\text {v i s u a l}} (\\cdot) \\right| \\right| \\left| \\left| f _ {\\text {t e x t}} (\\cdot) \\right| \\right|}, \\tag {2}\n$$\n\nwhere the comparison is done independently on the pixel level. The resulting similarity is comparable to the probability map of text queries. We apply dropout [52] to cross-attention during training to avoid over-reliance on attention.\n\nAutonomous Termination. To autonomously terminate skills, in order to hand control back to high-level planners, we add a virtual 'end' action signal prediction. Empirically, adding the end signal to only the end of the episode does not work, as the supervision is too sparse. Our proposed solution is to implement a buffer of end signal for every skill such that the last  $n = 10$  frames of the demonstrations carry the end signal. During deployment, we use a sliding window detector to terminate task execution if the end signal is greater than  $\\tau = 0.8$  for 10 consecutive predictions.\n\nb) WildLMa-Skill - Analytical Planning: In this paper, we learn all manipulation-related skills with imitation learning. For the base-only skill (i.e., navigating from a known\n\nlocation to another known location), we implement it with analytical planning.\n\n## C. WildLMa-Planner\n\nThe WildLMa-Skill module provides skills that can be composed for long-horizon execution, which is intentionally designed to be agnostic of the high-level planner. Here, we propose WildLMa-Planner, a simple LLM-based planner to show how learned skills can be composed.\n\nInitial Mapping. We implement a LiDAR-based SLAM system using FAST-LIO [62] and DLO [7] to obtain consistent robot pose estimation in the world frame. We manually annotate pose-level waypoints (e.g., stand in front of receptacles) and connectivity for task execution. The robot stands at every waypoint to capture images with its head camera. To automatically annotate the semantics of each waypoint, GPT4-V [37] provides high-level descriptions of images and lists of objects of each waypoint. We manually create abstract nodes (e.g., a room with multiple pose-level waypoints) to construct a hierarchical graph for searching. Note that off-the-shelf scene graph construction methods [20, 25, 36] can potentially replace this step.\n\nHierarchical Long-horizon Planning. We adopt a hierarchical coarse-to-fine approach to translate template-free commands into detailed, actionable robot skills.\n\nCoarse Planner. Using CoT [56], the coarse planner receives template-free instructions and decomposes them into individual tasks. For instance, the command 'clean the trash in the hallway' can be decomposed into tasks 'navigate to hallway', 'pick up the trash', and 'place trash in the trash bin'. We will release the detailed prompts.\n\nFine-grained Planner. The fine-grained planner invokes actionable skills at particular nodes given individual tasks generated by the coarse planner. The fine-grained planner has prior knowledge of the robot's skill library (shown in Fig. 3) and nodes constructed in the initial mapping stage. For each task, the agent uses a breadth-first search (BFS) approach to search nodes and identify the optimal goal node. During this stage the LLM acts as a heuristic evaluator, estimating the likelihood of a node being the most likely location related to\n\nonly for base\n\n|  Method | Tabletop Grasping |   | Button Pressing |   | Ground Grasping |   | Avg. Succ.  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|   |  I.D. | O.O.D. | I.D. | O.O.D | I.D. | O.O.D  |   |\n|  WildLMa (Ours) | 94.4% | 75% | 80% | 57.5% | 60% | 60% | 71.2%  |\n|  ACT (Mobile ALOHA) [17] | 77.8% | 19.4% | 55% | 25% | 60% | 30% | 40.8%  |\n|  OpenTV [9] | 88.9% | 77.8% | 75% | 25% | 50% | 50% | 64.4%  |\n|  VBC [31] | 50%* | 50%* | NA\u2020 | NA\u2020 | 43.8%* | 43.8%* | 46.9%  |\n|  GeFF [44] | 55.6%* | 55.6%* | NA\u2020 | NA\u2020 | NA\u2020 | NA\u2020 | 55.6%  |\n\nTABLE I: Success rate of autonomous skill execution. Imitation learning methods outperform RL [31] and zero-shot method [44] on comparable tasks. Both OpenTV and WildLMa achieve noticeably higher success rates in the challenging O.O.D. setting. \u2020: methods involve learned/manual policies that are not trivially applicable to the task settings. *: Method does not differentiate object sets and success rates are averaged on I.D. and O.O.D. object sets.\n\n|  Pipeline | Collect & Drop Trash | Shelf Rearrangement  |\n| --- | --- | --- |\n|  WildLMa (Ours) | 7/10 | 3/10  |\n|  ACT [17, 69] | 0/10 | 0/10  |\n\nthe task, based on the semantic context and objects present at the node. Once the target node is identified, the planner constructs a plan detailing the navigation and manipulation sequence drawn from the pre-defined skill library.\n\n# IV. EXPERIMENTS\n\nHardware Platforms. We use the Unitree B1 quadruped robot with a Unitree Z1 arm. We replace the beak-like default Z1 end effector with a 3D-printed parallel soft gripper, which was adapted from UMI-Gripper [12] to directly operate the gripper with gear rotations. For perception, an Azure Kinect camera is mounted on the robot's head, and an Intel Realsense D405 is used as the in-wrist camera. A LIVOX MID-360 LiDAR is installed at the robot's tail for enhanced localization during navigation.\n\nImplementation Details. The WildLMa-Skill module independently trains weights for each skill (with 30-60 demonstrations each acquired via tele-operation). The head/wrist RGB observations are processed through a CLIP [45] ViT-B/16 encoder with MaskCLIP [73] re-parameterization. Task-specific texts are then compared with the feature maps to generate cross-attention, where texts may differ in training sequences and testing run. For navigation between given waypoints, we implement a PD-based waypoint follower. WildLMa-Planner requires geometric annotations of nodes and edges. For efficiency, the spatial locations of nodes are annotated by operating the robot to turn 360 degrees during the initial scene scanning, and the edges are made between physically adjacent nodes with no obstacle in between.\n\nExperimental Protocol. We define two experiment settings to investigate the generalizability of skills learned via imitation learning [9, 13, 17]. The in-distribution (I.D.) setting tests the learned skills with backgrounds and object arrangements approximately similar to the training demonstrations. Note that, to make the setting more realistic, we do not enforce identical robot positioning and lighting conditions\n\nTABLE II: Evaluation of long-horizon execution. Given a few training demonstrations (10), WildLMa improves long-horizon task success rate via (1) improved generalizability of single skill and (2) divide-and-conquer.\n\n|  Backbone | In Dist. | Out of Dist. | Avg. Succ.  |\n| --- | --- | --- | --- |\n|  CLIP [45] | 83.3% | 69.4% | 76.4%  |\n|  ResNet [69]* | 77.8% | 19.4% | 48.6%  |\n|  DinoV2 [38] | 88.9% | 77.8% | 83.3%  |\n\nTABLE III: Ablation of different visual encoders pretrained with different objectives. The evaluation is done on the object-grasping tasks. *: we followed ACT [17, 69] to use ImageNet-pretrained ResNet-18 as the encoder, which has fewer parameters.\n\neven in I.D. settings. The Out-of-distribution (O.O.D.) setting permutes the testing objects (placement/texture), receptacles, and background environments for learned skills. Illustrations of the differences between these two settings can be found on the website.\n\nBaseline Implementation. Besides ablating design choices of our components, we implement several baselines to validate the efficacy of WildLMa. To compare with existing imitation learning methods, we choose Mobile ALOHA [17] which uses ACT [69] with ResNet-18 and OpenTV [9] using ACT and DinoV2 [38]. Unless specifically noticed, these baselines use the same training data as WildLMa. In addition, we compare two recent works in quadruped loco-manipulation [31, 44] to compare WildLMa against RL-based and zero-shot grasping methods. Note that both VBC [31] and GeFF [44] were designed for grasping, so they are not trivially applicable to non-prehensile manipulation such as button pressing.\n\n# A. Evaluation\n\nWe address important research questions in our evaluation:\n\n- What advantages does WildLMa-Skill have compared to existing baselines in quadruped manipulation? [A1, A2]\n- How does WildLMa-Planner perform in long-horizon execution? [A3]\n- Are the design choices (e.g., visual backbone and cross-attention) effective? [A4, A5]\n- Does whole-body control improve teleoperation? [A6]\n- What are the real-world applications of WildLMa? [A7]\n\nA1. WildLMa outperforms recent imitation learning baselines. From Tab. I, we can see that WildLMa achieves best overall success rate. Compared to vanilla ACT [17, 69], WildLMa achieves slightly better success rates on I.D. setting\n\n|  Metric | Whole-body (Ours) |   | Decoupled Control |   | W/o Whole-body (Arm Only)  |   |\n| --- | --- | --- | --- | --- | --- | --- |\n|   |  Ground Grasping | Rearrange Shelf | Ground Grasping | Rearrange Shelf | Ground Grasping | Rearrange Shelf  |\n|  Average Time | 21.87s | 27.25s | 37.35s | 29.81s | - | 27.88s  |\n|  Success Rate | 95% | 70% | 80% | 40% | 0% | 70%  |\n\nTABLE IV: Comparison of success rate and completion time for our whole-body controller, decoupled control with manual base pitching and arm control implemented via Unitree SDK, and arm-only policies. Four teleoperators are tasked to manipulate objects at various heights for three trials in each task.\n\n|  Camera | Tabletop Grasping | Button Pressing | Door Opening  |\n| --- | --- | --- | --- |\n|  Head + Wrist | 94.4% | 80% | 70%  |\n|  Head Only | 27.8% | 75% | 30%  |\n|  Wrist Only | 83.3% | 85% | 10%  |\n\nTABLE V: Ablation of input visual modality. Tasks that involve occlusion significantly benefit from multi-view setup.\n\n|  Backbone | In Dist. | Out of Dist. | Avg. Succ.  |\n| --- | --- | --- | --- |\n|  w/ cross-attention (Ours) | 94.4% | 75% | 84.7%  |\n|  w/o cross-attention | 83.3% | 69.4% | 76.4%  |\n\nTABLE VI: Ablation of cross-attention on the object-grasping tasks. Cross-attention improves both I.D. and O.O.D. setting by using additional task-specific information.\n\nand significantly better success rate on the O.O.D. setting. We reason this is because ResNet is vulnerable to changes in lighting and texture. OpenTV [9], on the other hand, shows more robustness to these adversarial conditions due to its use of the recent DinoV2 backbone [38], but slightly underperforms our method.\n\nA2. WildLMa outperforms RL and zero-shot baselines. Due to less reliance on real-world demonstrations, RL and zero-shot baselines demonstrate less performance gap between I.D. and the O.O.D. settings in Tab. I. As an RL-based method, VBC [31] suffers from sim2real gaps such as inaccurate contact modeling and cumulative sensor latencies. Therefore, VBC performs worse in real-world settings than its simulation counterpart. On the other hand, zero-shot modular methods such as GeFF [44] do not naturally exhibit corrective behavior like learning-based methods, which are vulnerable to errors compounding from different modules.\n\nA3. WildLMa is capable of handling long-horizon manipulation under perturbations. Tab. V validates the efficacy of WildLMa in handling long-horizon tasks under certain perturbations. We include videos on the website. Our experiments include 20 training sequences with variations in robot positioning, lighting, and object placement in both the training and testing time. ACT fails entirely for long-horizon tasks when trained directly on a few sequences of demonstrations. On the other hand, WildLMa successfully learns generalizable skills from a limited number of demonstrations to achieve better success rates for long-horizon execution.\n\nA4. Pre-trained Visual Backbones improve skill generalizability. We ablate the choice of visual backbones in Tab. III. CLIP [45] is the simple application of CLIP features without cross-attention. While different backbones perform similarly in the I.D. setting, we see that frozen large\n\n![img-5.jpeg](img-5.jpeg)\nFig. 4: Qualitative illustrations of some evaluated tasks.\n\nmodels [38, 45] perform much better in the O.O.D. setting. A5. Cross-attention significantly improves O.O.D. imitation learning. Tab. VI shows the proposed cross-attention improves both the I.D. and O.O.D. performance of CLIP [45] module by introducing additional task-specific text prompts. A6. Whole-body controllers enable efficient VR teleoperation of quadruped robots. The motivation for combining whole-body control and teleoperation is to improve teleoperation efficiency. To validate this point, we report the statistics of teleoperation in Tab. IV, which shows our learning-based controller outperforms the decoupled analytical controller from Unitree SDK. Since these tasks require reaching objects at various heights (toys and books at different levels of storage), teleoperation without whole-body control fails to grasp from the ground due to the limited workspace.\n\nA7. WildLMa allows the robot to learn diverse tasks. Besides qualitative samples in Fig. 1, we provide more videos of our robot working on different practical tasks in the supplementary video and the website.\n\n# V. CONCLUSION\n\nIn this paper, we present WildLMa, a modular framework that includes (1) WildLMa-Skill, which implements a library of generalizable visuomotor skills that improve ACT [69] for learning generalizable imitation learning skills; and (2) WildLMa-Planner, an interface that enables interactions between imitation learning skills and LLM planner to support long-horizon task execution. Furthermore, we deploy this framework on a quadruped robot controlled by a whole-body controller, which allows us to efficiently collect demonstration data and support extended workspace for diverse tasks. In summary, WildLMa implements practical, generalizable skills, and long-horizon manipulation, which we hope will motivate future research toward in-the-wild mobile manipulation that facilitates real-world deployment of robots.\n\nReferences\n\n- [1] M. Ahn et al., \u201cDo as i can, not as i say: Grounding language in robotic affordances,\u201d *arXiv preprint arXiv:2204.01691*, 2022.\n- [2] E. Arcari et al., \u201cBayesian multi-task learning mpc for robotic mobile manipulation,\u201d *IEEE Robotics and Automation Letters*, 2023.\n- [3] C. D. Bellicoso et al., \u201cAlma-articulated locomotion and manipulation for a torque-controllable robot,\u201d in *2019 International conference on robotics and automation (ICRA)*, 2019.\n- [4] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani, \u201cTrack2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation,\u201d in *ECCV*, 2024.\n- [5] M. Bojarski, \u201cEnd to end learning for self-driving cars,\u201d in *arXiv preprint arXiv:1604.07316*, 2016.\n- [6] M. Chang et al., \u201cGoat: Go to any thing,\u201d in *RSS*, 2024.\n- [7] K. Chen, B. T. Lopez, A.-a. Agha-mohammadi, and A. Mehta, \u201cDirect lidar odometry: Fast localization with dense point clouds,\u201d *IEEE Robotics and Automation Letters*, 2022.\n- [8] A.-C. Cheng et al., \u201cSpatialrgpt: Grounded spatial reasoning in vision language model,\u201d *arXiv preprint arXiv:2406.01584*, 2024.\n- [9] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang, \u201cOpen-television: Teleoperation with immersive active visual feedback,\u201d in *CoRL*, 2024.\n- [10] X. Cheng, K. Shi, A. Agarwal, and D. Pathak, \u201cExtreme parkour with legged robots,\u201d in *ICRA*, 2024.\n- [11] C. Chi et al., \u201cDiffusion policy: Visuomotor policy learning via action diffusion,\u201d in *RSS*, 2023.\n- [12] C. Chi et al., \u201cUniversal manipulation interface: In-the-wild robot teaching without in-the-wild robots,\u201d in *RSS*, 2024.\n- [13] R. Ding et al., \u201cBunny-visionpro: Real-time bimanual dexterous teleoperation for imitation learning,\u201d *arXiv preprint arXiv:2407.03162*, 2024.\n- [14] H.-S. Fang, C. Wang, M. Gou, and C. Lu, \u201cGraspnet-1billion: A large-scale benchmark for general object grasping,\u201d in *CVPR*, 2020.\n- [15] Z. Fu, X. Cheng, and D. Pathak, \u201cDeep whole-body control: Learning a unified policy for manipulation and locomotion,\u201d in *Conference on Robot Learning*, 2023.\n- [16] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, \u201cHumanplus: Humanoid shadowing and imitation from humans,\u201d in *CoRL*, 2024.\n- [17] Z. Fu, T. Z. Zhao, and C. Finn, \u201cMobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation,\u201d *arXiv*, 2024.\n- [18] C. R. Garrett, T. Lozano-P\u00e9rez, and L. P. Kaelbling, \u201cPddlstream: Integrating symbolic planners and black-box samplers via optimistic adaptive planning,\u201d *arXiv*, 2020.\n- [19] J. Gu, D. S. Chaplot, H. Su, and J. Malik, \u201cMulti-skill mobile manipulation for object rearrangement,\u201d in *The Eleventh International Conference on Learning Representations*, 2023.\n- [20] Q. Gu et al., \u201cConceptgraphs: Open-vocabulary 3d scene graphs for perception and planning,\u201d in *ICRA*, 2024.\n- [21] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, \u201cOpen-vocabulary object detection via vision and language knowledge distillation,\u201d *arXiv preprint arXiv:2104.13921*, 2021.\n- [22] H. Ha, Y. Gao, Z. Fu, J. Tan, and S. Song, \u201cUmi on legs: Making manipulation policies mobile with manipulation-centric whole-body controllers,\u201d in *CoRL*, 2024.\n- [23] T. He et al., \u201cOmnih2o: Universal and dexterous human-to-humanoid whole-body teleoperation and learning,\u201d in *CoRL*, 2024.\n- [24] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, \u201cLanguage models as zero-shot planners: Extracting actionable knowledge for embodied agents,\u201d in *International conference on machine learning*, 2022.\n- [25] N. Hughes, Y. Chang, and L. Carlone, \u201cHydra: A real-time spatial perception system for 3d scene graph construction and optimization,\u201d *arXiv preprint arXiv:2201.13360*, 2022.\n- [26] A. Iyer et al., \u201cOpen teach: A versatile teleoperation system for robotic manipulation,\u201d *arXiv preprint arXiv:2403.07870*, 2024.\n- [27] M. Ji, R.-Z. Qiu, X. Zou, and X. Wang, \u201cGraspsplats: Efficient manipulation with 3d feature splatting,\u201d in *CoRL*, 2024.\n- [28] A. Kirillov et al., \u201cSegment anything,\u201d in *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 2023.\n- [29] T. Lew et al., \u201cRobotic table wiping via reinforcement learning and whole-body trajectory optimization,\u201d in *2023 IEEE International Conference on Robotics and Automation (ICRA)*, 2023.\n- [30] Z. Liang, Y. Mu, H. Ma, M. Tomizuka, M. Ding, and P. Luo, \u201cSkilldiffuser: Interpretable hierarchical planning via skill abstractions in diffusion-based task execution,\u201d in *CVPR*, 2024.\n- [31] M. Liu et al., \u201cVisual whole-body control for legged loco-manipulation,\u201d in *CoRL*, 2024.\n- [32] P. Liu, Y. Orru, C. Paxton, N. M. M. Shafiullah, and L. Pinto, \u201cOk-robot: What really matters in integrating open-knowledge models for robotics,\u201d *arXiv preprint arXiv:2401.12202*, 2024.\n- [33] P. Liu et al., \u201cDynamem: Online dynamic spatio-semantic memory for open world mobile manipulation,\u201d *arXiv preprint arXiv:2411.04999*, 2024.\n- [34] S. Liu et al., \u201cGrounding dino: Marrying dino with grounded pre-training for open-set object detection,\u201d *arXiv preprint arXiv:2303.05499*, 2023.\n- [35] Y. Ma, F. Farshidian, T. Miki, J. Lee, and M. Hutter, \u201cCombining learning-based locomotion policy with\n\nmodel-based manipulation for legged mobile manipulators,\u201d IEEE Robotics and Automation Letters, 2022.\n- [36] D. Maggio et al., \u201cClio: Real-time task-driven open-set 3d scene graphs,\u201d arXiv preprint arXiv:2404.13696, 2024.\n- [37] OpenAI, \u201cGpt-4 technical report,\u201d OpenAI, Tech. Rep., 2023.\n- [38] M. Oquab et al., \u201cDinov2: Learning robust visual features without supervision,\u201d arXiv preprint arXiv:2304.07193, 2023.\n- [39] A. Padalkar et al., \u201cOpen x-embodiment: Robotic learning datasets and rt-x models,\u201d arXiv preprint arXiv:2310.08864, 2023.\n- [40] G. Pan et al., \u201cRoboduet: A framework affording mobile-manipulation and cross-embodiment,\u201d arXiv preprint arXiv:2403.17367, 2024.\n- [41] D. A. Pomerleau, \u201cAlvinn: An autonomous land vehicle in a neural network,\u201d in Advances in neural information processing systems, 1988.\n- [42] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, \u201cLearning force control for legged manipulation,\u201d in ICRA, 2024.\n- [43] Y. Qin et al., \u201cAnyteleop: A general vision-based dexterous robot arm-hand teleoperation system,\u201d arXiv preprint arXiv:2307.04577, 2023.\n- [44] R.-Z. Qiu et al., \u201cLearning generalizable feature fields for mobile manipulation,\u201d arXiv preprint arXiv:2403.07563, 2024.\n- [45] A. Radford et al., \u201cLearning transferable visual models from natural language supervision,\u201d in ICML, PMLR, 2021.\n- [46] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suenderhauf, \u201cSayplan: Grounding large language models using 3d scene graphs for scalable robot task planning,\u201d in CoRL, 2023.\n- [47] Ros moveit motion planning framework, https://moveit.ros.org/, Accessed: 2024-09-13.\n- [48] N. M. M. Shafiullah et al., \u201cOn bringing robots home,\u201d arXiv preprint arXiv:2311.16098, 2023.\n- [49] W. Shen, G. Yang, A. Yu, J. Wong, L. P. Kaelbling, and P. Isola, \u201cDistilled feature fields enable few-shot language-guided manipulation,\u201d in CoRL, 2023.\n- [50] J.-P. Sleiman, F. Farshidian, and M. Hutter, \u201cVersatile multicontact planning and control for legged loco-manipulation,\u201d Science Robotics, 2023.\n- [51] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su, \u201cLlm-planner: Few-shot grounded planning for embodied agents with large language models,\u201d in ICCV, 2023.\n- [52] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: A simple way to prevent neural networks from overfitting,\u201d The journal of machine learning research, 2014.\n- [53] S. Srivastava, E. Fang, L. Riano, R. Chitnis, S. Russell, and P. Abbeel, \u201cCombined task and motion planning through an extensible planner-independent interface layer,\u201d in ICRA, 2014.\n- [54] Stretch open source mobile manipulator - hello robot, https://hello-robot.com/stretch-3-product, Accessed: 2024-09-01.\n- [55] C. Sun et al., \u201cFully autonomous real-world reinforcement learning with applications to mobile manipulation,\u201d in Conference on Robot Learning, 2022.\n- [56] J. Wei et al., \u201cChain-of-thought prompting elicits reasoning in large language models,\u201d NeurIPS, 2022.\n- [57] J. Wong et al., \u201cError-aware imitation learning from teleoperation data for mobile manipulation,\u201d in Conference on Robot Learning, 2022.\n- [58] J. Wu et al., \u201cTidybot: Personalized robot assistance with large language models,\u201d Autonomous Robots, 2023.\n- [59] P. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, \u201cGello: A general, low-cost, and intuitive teleoperation framework for robot manipulators,\u201d arXiv preprint arXiv:2309.13037, 2023.\n- [60] F. Xia, C. Li, R. Mart\u00edn-Mart\u00edn, O. Litany, A. Toshev, and S. Savarese, \u201cRelmogen: Integrating motion generation in reinforcement learning for mobile manipulation,\u201d in 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021.\n- [61] H. Xiong, R. Mendonca, K. Shaw, and D. Pathak, \u201cAdaptive mobile manipulation for articulated objects in the open world,\u201d arXiv preprint arXiv:2401.14403, 2024.\n- [62] W. Xu and F. Zhang, \u201cFast-lio: A fast, robust lidar-inertial odometry package by tightly-coupled iterated kalman filter,\u201d IEEE Robotics and Automation Letters, 2021.\n- [63] R. Yang et al., \u201cGeneralized animal imitator: Agile locomotion with versatile motion prior,\u201d arXiv preprint arXiv:2310.01408, 2023.\n- [64] S. Yang et al., \u201cAce: A cross-platform visual-exoskeletons system for low-cost dexterous teleoperation,\u201d in CoRL, 2024.\n- [65] S. Yenamandra et al., \u201cHomerobot: Open-vocabulary mobile manipulation,\u201d arXiv preprint arXiv:2306.11565, 2023.\n- [66] N. Yokoyama et al., \u201cAsc: Adaptive skill coordination for robotic mobile manipulation,\u201d IEEE Robotics and Automation Letters, 2023.\n- [67] J. Zhang et al., \u201cGamma: Graspability-aware mobile manipulation policy learning based on online grasping pose fusion,\u201d arXiv preprint arXiv:2309.15459, 2023.\n- [68] K. Zhang, B. Li, K. Hauser, and Y. Li, \u201cAdaptigraph: Material-adaptive graph-based neural dynamics for robotic manipulation,\u201d in RSS, 2024.\n- [69] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, \u201cLearning fine-grained bimanual manipulation with low-cost hardware,\u201d in arXiv preprint arXiv:2304.13705, 2023.\n- [70] T. Z. Zhao et al., \u201cAloha unleashed: A simple recipe for robot dexterity,\u201d in CoRL, 2024.\n- [71] P. Zhi et al., \u201cClosed-loop open-vocabulary mobile manipulation with gpt-4v,\u201d arXiv preprint arXiv:2404.10220, 2024.\n\n[72] C. Zhou, C. Peers, Y. Wan, R. Richardson, and D. Kanoulas, \u201cTeleman: Teleoperation for legged robot loco-manipulation using wearable imu-based motion capture,\u201d *arXiv preprint arXiv:2209.10314*, 2022.\n- [73] C. Zhou, C. C. Loy, and B. Dai, \u201cExtract free dense labels from clip,\u201d in *ECCV*, 2022.\n- [74] S. Zimmermann, R. Poranne, and S. Coros, \u201cGo fetch!-dynamic grasps using boston dynamics spot with external robotic arm,\u201d in *2021 IEEE International Conference on Robotics and Automation (ICRA)*, 2021.",
  "annotations": [
    {
      "id": "afceba16-472c-4b88-a126-bc77513b8861",
      "type": "diagram",
      "content": "The image depicts a series of photographs of a quadruped robot in various environments and activities. The top section is labeled 'Franklin Antonio Hall' and shows the robot indoors, possibly in a laboratory setting, performing different tasks such as walking on a treadmill and interacting with objects. The middle section is divided into two routes: Route 1 and Route 2. Route 1 shows the robot navigating through grassland, while Route 2 shows the robot moving through a sandy area. Each route includes multiple images capturing different stages of the robot's movement and interaction with the terrain. The overall image illustrates the robot's versatility and adaptability in different environments.",
      "page_number": 1,
      "position": {
        "image_id": "img-0.jpeg",
        "bbox": {
          "top_left_x": 151,
          "top_left_y": 547,
          "bottom_right_x": 911,
          "bottom_right_y": 926
        }
      },
      "context": null,
      "confidence": null
    },
    {
      "id": "f835f2c1-3077-4892-a1e7-0df90b021342",
      "type": "diagram",
      "content": "The image illustrates a process of robot learning through imitation. It shows a sequence of steps starting with tele-operation data, where a human operator controls a robot to perform various tasks. The visual observation section depicts the robot's actions and environment, including base velocity and 6 degrees of freedom end-effector (6DoF EE) movements. The collected data is then used in an imitation learning process, enabling the robot to perform tasks such as pressing buttons, opening or pushing doors, placing objects, dropping objects, and whole-body grasping.",
      "page_number": 1,
      "position": {
        "image_id": "img-1.jpeg",
        "bbox": {
          "top_left_x": 911,
          "top_left_y": 550,
          "bottom_right_x": 1543,
          "bottom_right_y": 924
        }
      },
      "context": null,
      "confidence": null
    },
    {
      "id": "81de3b27-6ff4-4fcd-a255-80c5a0f54a65",
      "type": "diagram",
      "content": "The image depicts a detailed schematic of a robotic control system. It is divided into two main parts: (a) WildLMa - Skill and (b) Whole-body Controller. The left part (a) shows the process of extracting visual CLIP features from task-specific texts and images, which are then processed through a cross-attention mechanism. The right part (b) illustrates the whole-body controller of a robot, which uses these features along with the robot's state to generate actions. The system involves several components such as a frozen CLIP model, action chunking transformers, and multi-layer perceptrons (MLPs) that aggregate and process the data to control the robot's movements and interactions with its environment.",
      "page_number": 3,
      "position": {
        "image_id": "img-2.jpeg",
        "bbox": {
          "top_left_x": 180,
          "top_left_y": 112,
          "bottom_right_x": 1497,
          "bottom_right_y": 583
        }
      },
      "context": null,
      "confidence": null
    },
    {
      "id": "09b7694d-f860-49ac-8591-8b388784f01a",
      "type": "diagram",
      "content": "The image depicts a flowchart illustrating the process of planning and executing a task using a hierarchical planning system. The system includes a coarse planner and a fine-grained planner. The coarse planner breaks down a high-level task, such as 'Please clean the trash in the hallway,' into intermediate steps: 'Go to Table in the Hallway,' 'Pick up the trash,' and 'Place trash in the trash bin.' The fine-grained planner further refines these steps into actionable skills: 'Nav(Table),' 'GraspTable(Bottle),' 'Nav(TrashBin),' and 'Place(TrashBin).' The flowchart also includes a 'Fine-grained Planner' section that takes inputs like individual tasks, pre-built nodes, and a skillset, and outputs actionable skills, node descriptions, and node information. The skillset includes actions like 'GraspFloor(obj),' 'GraspTable(obj),' 'Press(obj),' 'Place(obj),' 'Nav(obj),' and 'PushDoor.' The image also shows pre-built nodes such as 'Floor 2 Robotic Lab,' 'Floor 1 Hallway,' 'Trash Bin,' 'Bottle on the Table,' and 'Backpack,' which are manually annotated. The overall concept demonstrates how a hierarchical planning system can break down complex tasks into simpler, executable actions.",
      "page_number": 4,
      "position": {
        "image_id": "img-3.jpeg",
        "bbox": {
          "top_left_x": 168,
          "top_left_y": 143,
          "bottom_right_x": 1696,
          "bottom_right_y": 618
        }
      },
      "context": null,
      "confidence": null
    },
    {
      "id": "2d154384-01e9-4db2-9b7e-049b6b65b41e",
      "type": "handwritten_note",
      "content": "The image contains handwritten text in red ink. The text reads: 'What is H & W here in img. 2? Reger?'. The handwriting is cursive and somewhat slanted, making it appear informal and personal.",
      "page_number": 4,
      "position": {
        "image_id": "img-4.jpeg",
        "bbox": {
          "top_left_x": 1,
          "top_left_y": 734,
          "bottom_right_x": 132,
          "bottom_right_y": 1056
        }
      },
      "context": null,
      "confidence": null
    },
    {
      "id": "fae1e9ba-7614-4419-a23f-4d1ef24dfa86",
      "type": "diagram",
      "content": "The image shows a quadruped robot performing various tasks. The robot is equipped with robotic arms and is shown in four different scenarios: (a) Tabletop Grasping, where the robot is grasping an object on a table; (b) Ground Grasping, where the robot is picking up an object from the ground; (a) Door Button Pressing, where the robot is pressing a button on a door; and (a) Shelf Rearrangement, where the robot is interacting with objects on a shelf. The robot demonstrates its capability to perform diverse manipulation tasks in different environments.",
      "page_number": 6,
      "position": {
        "image_id": "img-5.jpeg",
        "bbox": {
          "top_left_x": 882,
          "top_left_y": 446,
          "bottom_right_x": 1533,
          "bottom_right_y": 814
        }
      },
      "context": null,
      "confidence": null
    }
  ],
  "raw_file_hash": "984b13cc1648a4ece82a9659a30d392d24086baa94d59cbb6218a45c64f68e1c",
  "asset_paths": [
    "/Users/dpickem/workspace/project_second_brain/scripts/test_data/sample_paper_wild_ma.pdf"
  ],
  "processing_status": "pending",
  "error_message": null,
  "obsidian_path": null,
  "tags": [],
  "metadata": {
    "ocr_model": "mistral/mistral-ocr-latest",
    "page_count": 9,
    "summary": "The document presents WildLMa, a framework for in-the-wild manipulation with a quadruped robot. It combines a whole-body controller and imitation learning for effective single-skill learning. The paper proposes WildLMa with three components to address issues in mobile manipulation: adaptation of learned low-level controllers, a library of generalizable visuomotor skills, and an interface for learned skills. The framework is demonstrated to achieve higher grasping success rates and is evaluated through practical robot applications such as cleaning trash in university hallways or outdoor terrains.",
    "languages": [
      "en"
    ],
    "llm_cost_usd": 0.009386,
    "llm_api_calls": 2,
    "annotation_counts": {
      "highlights": 0,
      "handwritten_notes": 1,
      "comments": 0,
      "diagrams": 5,
      "underlines": 0,
      "total": 6
    }
  }
}