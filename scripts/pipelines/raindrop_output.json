[
  {
    "id": "4aacd3d8-30b4-40d6-ac36-43c1ce369fde",
    "source_type": "article",
    "source_url": "http://nlp.seas.harvard.edu/annotated-transformer/",
    "source_file_path": null,
    "title": "The Annotated Transformer",
    "authors": [
      "Unknown"
    ],
    "created_at": "2024-11-13T07:59:12.965000+00:00",
    "ingested_at": "2026-01-01T20:24:02.365182",
    "full_text": "The Transformer has been on a lot of people\u2019s minds over the last year five years. This post presents an annotated version of the paper in the form of a line-by-line implementation. It reorders and deletes some sections from the original paper and adds comments throughout. This document itself is a working notebook, and should be a completely usable implementation. Code is available here.\n# !pip install -r requirements.txt\n# # Uncomment for colab\n# #\n# !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n# !python -m spacy download de_core_news_sm\n# !python -m spacy download en_core_web_sm\nimport os\nfrom os.path import exists\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import log_softmax, pad\nimport math\nimport copy\nimport time\nfrom torch.optim.lr_scheduler import LambdaLR\nimport pandas as pd\nimport altair as alt\nfrom torchtext.data.functional import to_map_style_dataset\nfrom torch.utils.data import DataLoader\nfrom torchtext.vocab import build_vocab_from_iterator\nimport torchtext.datasets as datasets\nimport spacy\nimport GPUtil\nimport warnings\nfrom torch.utils.data.distributed import DistributedSampler\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n# Set to False to skip notebook execution (e.g. for debugging)\nwarnings.filterwarnings(\"ignore\")\nRUN_EXAMPLES = True\n# Some convenience helper functions used throughout the notebook\ndef is_interactive_notebook():\nreturn __name__ == \"__main__\"\ndef show_example(fn, args=[]):\nif __name__ == \"__main__\" and RUN_EXAMPLES:\nreturn fn(*args)\ndef execute_example(fn, args=[]):\nif __name__ == \"__main__\" and RUN_EXAMPLES:\nfn(*args)\nclass DummyOptimizer(torch.optim.Optimizer):\ndef __init__(self):\nself.param_groups = [{\"lr\": 0}]\nNone\ndef step(self):\nNone\ndef zero_grad(self, set_to_none=False):\nNone\nclass DummyScheduler:\ndef step(self):\nNone\nMy comments are blockquoted. The main text is all from the paper itself.\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.\nMost competitive neural sequence transduction models have an encoder-decoder structure (cite). Here, the encoder maps an input sequence of symbol representations (x_1, ..., x_n) to a sequence of continuous representations \\mathbf{z} = (z_1, ..., z_n). Given \\mathbf{z}, the decoder then generates an output sequence (y_1,...,y_m) of symbols one element at a time. At each step the model is auto-regressive (cite), consuming the previously generated symbols as additional input when generating the next.\nclass EncoderDecoder(nn.Module):\n\"\"\"\nA standard Encoder-Decoder architecture. Base for this and many\nother models.\n\"\"\"\ndef __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\nsuper(EncoderDecoder, self).__init__()\nself.encoder = encoder\nself.decoder = decoder\nself.src_embed = src_embed\nself.tgt_embed = tgt_embed\nself.generator = generator\ndef forward(self, src, tgt, src_mask, tgt_mask):\n\"Take in and process masked src and target sequences.\"\nreturn self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\ndef encode(self, src, src_mask):\nreturn self.encoder(self.src_embed(src), src_mask)\ndef decode(self, memory, src_mask, tgt, tgt_mask):\nreturn self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\nclass Generator(nn.Module):\n\"Define standard linear + softmax generation step.\"\ndef __init__(self, d_model, vocab):\nsuper(Generator, self).__init__()\nself.proj = nn.Linear(d_model, vocab)\ndef forward(self, x):\nreturn log_softmax(self.proj(x), dim=-1)\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\nThe encoder is composed of a stack of N=6 identical layers.\ndef clones(module, N):\n\"Produce N identical layers.\"\nreturn nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\nclass Encoder(nn.Module):\n\"Core encoder is a stack of N layers\"\ndef __init__(self, layer, N):\nsuper(Encoder, self).__init__()\nself.layers = clones(layer, N)\nself.norm = LayerNorm(layer.size)\ndef forward(self, x, mask):\n\"Pass the input (and mask) through each layer in turn.\"\nfor layer in self.layers:\nx = layer(x, mask)\nreturn self.norm(x)\nWe employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite).\nclass LayerNorm(nn.Module):\n\"Construct a layernorm module (See citation for details).\"\ndef __init__(self, features, eps=1e-6):\nsuper(LayerNorm, self).__init__()\nself.a_2 = nn.Parameter(torch.ones(features))\nself.b_2 = nn.Parameter(torch.zeros(features))\nself.eps = eps\ndef forward(self, x):\nmean = x.mean(-1, keepdim=True)\nstd = x.std(-1, keepdim=True)\nreturn self.a_2 * (x - mean) / (std + self.eps) + self.b_2\nThat is, the output of each sub-layer is \\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x)), where \\mathrm{Sublayer}(x) is the function implemented by the sub-layer itself. We apply dropout (cite) to the output of each sub-layer, before it is added to the sub-layer input and normalized.\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_{\\text{model}}=512.\nclass SublayerConnection(nn.Module):\n\"\"\"\nA residual connection followed by a layer norm.\nNote for code simplicity the norm is first as opposed to last.\n\"\"\"\ndef __init__(self, size, dropout):\nsuper(SublayerConnection, self).__init__()\nself.norm = LayerNorm(size)\nself.dropout = nn.Dropout(dropout)\ndef forward(self, x, sublayer):\n\"Apply residual connection to any sublayer with the same size.\"\nreturn x + self.dropout(sublayer(self.norm(x)))\nEach layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\nclass EncoderLayer(nn.Module):\n\"Encoder is made up of self-attn and feed forward (defined below)\"\ndef __init__(self, size, self_attn, feed_forward, dropout):\nsuper(EncoderLayer, self).__init__()\nself.self_attn = self_attn\nself.feed_forward = feed_forward\nself.sublayer = clones(SublayerConnection(size, dropout), 2)\nself.size = size\ndef forward(self, x, mask):\n\"Follow Figure 1 (left) for connections.\"\nx = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\nreturn self.sublayer[1](x, self.feed_forward)\nThe decoder is also composed of a stack of N=6 identical layers.\nclass Decoder(nn.Module):\n\"Generic N layer decoder with masking.\"\ndef __init__(self, layer, N):\nsuper(Decoder, self).__init__()\nself.layers = clones(layer, N)\nself.norm = LayerNorm(layer.size)\ndef forward(self, x, memory, src_mask, tgt_mask):\nfor layer in self.layers:\nx = layer(x, memory, src_mask, tgt_mask)\nreturn self.norm(x)\nIn addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\nclass DecoderLayer(nn.Module):\n\"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\ndef __init__(self, size, self_attn, src_attn, feed_forward, dropout):\nsuper(DecoderLayer, self).__init__()\nself.size = size\nself.self_attn = self_attn\nself.src_attn = src_attn\nself.feed_forward = feed_forward\nself.sublayer = clones(SublayerConnection(size, dropout), 3)\ndef forward(self, x, memory, src_mask, tgt_mask):\n\"Follow Figure 1 (right) for connections.\"\nm = memory\nx = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\nx = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\nreturn self.sublayer[2](x, self.feed_forward)\nWe also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\ndef subsequent_mask(size):\n\"Mask out subsequent positions.\"\nattn_shape = (1, size, size)\nsubsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\ntorch.uint8\n)\nreturn subsequent_mask == 0\nBelow the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training.\ndef example_mask():\nLS_data = pd.concat(\n[\npd.DataFrame(\n{\n\"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n\"Window\": y,\n\"Masking\": x,\n}\n)\nfor y in range(20)\nfor x in range(20)\n]\n)\nreturn (\nalt.Chart(LS_data)\n.mark_rect()\n.properties(height=250, width=250)\n.encode(\nalt.X(\"Window:O\"),\nalt.Y(\"Masking:O\"),\nalt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n)\n.interactive()\n)\nshow_example(example_mask)\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nWe call our particular attention \u201cScaled Dot-Product Attention\u201d. The input consists of queries and keys of dimension d_k, and values of dimension d_v. We compute the dot products of the query with all keys, divide each by \\sqrt{d_k}, and apply a softmax function to obtain the weights on the values.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:\n\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\ndef attention(query, key, value, mask=None, dropout=None):\n\"Compute 'Scaled Dot Product Attention'\"\nd_k = query.size(-1)\nscores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\nif mask is not None:\nscores = scores.masked_fill(mask == 0, -1e9)\np_attn = scores.softmax(dim=-1)\nif dropout is not None:\np_attn = dropout(p_attn)\nreturn torch.matmul(p_attn, value), p_attn\nThe two most commonly used attention functions are additive attention (cite), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \\frac{1}{\\sqrt{d_k}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\nWhile for small values of d_k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d_k (cite). We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients (To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i, has mean 0 and variance d_k.). To counteract this effect, we scale the dot products by \\frac{1}{\\sqrt{d_k}}.\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\ \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\nWhere the projections are parameter matrices W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v} and W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}.\nIn this work we employ h=8 parallel attention layers, or heads. For each of these we use d_k=d_v=d_{\\text{model}}/h=64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\nclass MultiHeadedAttention(nn.Module):\ndef __init__(self, h, d_model, dropout=0.1):\n\"Take in model size and number of heads.\"\nsuper(MultiHeadedAttention, self).__init__()\nassert d_model % h == 0\n# We assume d_v always equals d_k\nself.d_k = d_model // h\nself.h = h\nself.linears = clones(nn.Linear(d_model, d_model), 4)\nself.attn = None\nself.dropout = nn.Dropout(p=dropout)\ndef forward(self, query, key, value, mask=None):\n\"Implements Figure 2\"\nif mask is not None:\n# Same mask applied to all h heads.\nmask = mask.unsqueeze(1)\nnbatches = query.size(0)\n# 1) Do all the linear projections in batch from d_model => h x d_k\nquery, key, value = [\nlin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\nfor lin, x in zip(self.linears, (query, key, value))\n]\n# 2) Apply attention on all the projected vectors in batch.\nx, self.attn = attention(\nquery, key, value, mask=mask, dropout=self.dropout\n)\n# 3) \"Concat\" using a view and apply a final linear.\nx = (\nx.transpose(1, 2)\n.contiguous()\n.view(nbatches, -1, self.h * self.d_k)\n)\ndel query\ndel key\ndel value\nreturn self.linears[-1](x)\nThe Transformer uses multi-head attention in three different ways: 1) In \u201cencoder-decoder attention\u201d layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as (cite).\nThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -\\infty) all values in the input of the softmax which correspond to illegal connections.\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d_{\\text{model}}=512, and the inner-layer has dimensionality d_{ff}=2048.\nclass PositionwiseFeedForward(nn.Module):\n\"Implements FFN equation.\"\ndef __init__(self, d_model, d_ff, dropout=0.1):\nsuper(PositionwiseFeedForward, self).__init__()\nself.w_1 = nn.Linear(d_model, d_ff)\nself.w_2 = nn.Linear(d_ff, d_model)\nself.dropout = nn.Dropout(dropout)\ndef forward(self, x):\nreturn self.w_2(self.dropout(self.w_1(x).relu()))\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d_{\\text{model}}. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to (cite). In the embedding layers, we multiply those weights by \\sqrt{d_{\\text{model}}}.\nclass Embeddings(nn.Module):\ndef __init__(self, d_model, vocab):\nsuper(Embeddings, self).__init__()\nself.lut = nn.Embedding(vocab, d_model)\nself.d_model = d_model\ndef forward(self, x):\nreturn self.lut(x) * math.sqrt(self.d_model)\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \u201cpositional encodings\u201d to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d_{\\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (cite).\nIn this work, we use sine and cosine functions of different frequencies:\nPE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})\nPE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\\pi to 10000 \\cdot 2\\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE_{pos+k} can be represented as a linear function of PE_{pos}.\nIn addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_{drop}=0.1.\nclass PositionalEncoding(nn.Module):\n\"Implement the PE function.\"\ndef __init__(self, d_model, dropout, max_len=5000):\nsuper(PositionalEncoding, self).__init__()\nself.dropout = nn.Dropout(p=dropout)\n# Compute the positional encodings once in log space.\npe = torch.zeros(max_len, d_model)\nposition = torch.arange(0, max_len).unsqueeze(1)\ndiv_term = torch.exp(\ntorch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n)\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\npe = pe.unsqueeze(0)\nself.register_buffer(\"pe\", pe)\ndef forward(self, x):\nx = x + self.pe[:, : x.size(1)].requires_grad_(False)\nreturn self.dropout(x)\nBelow the positional encoding will add in a sine wave based on position. The frequency and offset of the wave is different for each dimension.\ndef example_positional():\npe = PositionalEncoding(20, 0)\ny = pe.forward(torch.zeros(1, 100, 20))\ndata = pd.concat(\n[\npd.DataFrame(\n{\n\"embedding\": y[0, :, dim],\n\"dimension\": dim,\n\"position\": list(range(100)),\n}\n)\nfor dim in [4, 5, 6, 7]\n]\n)\nreturn (\nalt.Chart(data)\n.mark_line()\n.properties(width=800)\n.encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n.interactive()\n)\nshow_example(example_positional)\nWe also experimented with using learned positional embeddings (cite) instead, and found that the two versions produced nearly identical results. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\nHere we define a function from hyperparameters to a full model.\ndef make_model(\nsrc_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n):\n\"Helper: Construct a model from hyperparameters.\"\nc = copy.deepcopy\nattn = MultiHeadedAttention(h, d_model)\nff = PositionwiseFeedForward(d_model, d_ff, dropout)\nposition = PositionalEncoding(d_model, dropout)\nmodel = EncoderDecoder(\nEncoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\nDecoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\nnn.Sequential(Embeddings(d_model, src_vocab), c(position)),\nnn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\nGenerator(d_model, tgt_vocab),\n)\n# This was important from their code.\n# Initialize parameters with Glorot / fan_avg.\nfor p in model.parameters():\nif p.dim() > 1:\nnn.init.xavier_uniform_(p)\nreturn model\nHere we make a forward step to generate a prediction of the model. We try to use our transformer to memorize the input. As you will see the output is randomly generated due to the fact that the model is not trained yet. In the next tutorial we will build the training function and try to train our model to memorize the numbers from 1 to 10.\ndef inference_test():\ntest_model = make_model(11, 11, 2)\ntest_model.eval()\nsrc = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nsrc_mask = torch.ones(1, 1, 10)\nmemory = test_model.encode(src, src_mask)\nys = torch.zeros(1, 1).type_as(src)\nfor i in range(9):\nout = test_model.decode(\nmemory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n)\nprob = test_model.generator(out[:, -1])\n_, next_word = torch.max(prob, dim=1)\nnext_word = next_word.data[0]\nys = torch.cat(\n[ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n)\nprint(\"Example Untrained Model Prediction:\", ys)\ndef run_tests():\nfor _ in range(10):\ninference_test()\nshow_example(run_tests)\nExample Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\nExample Untrained Model Prediction: tensor([[0, 3, 4, 4, 4, 4, 4, 4, 4, 4]])\nExample Untrained Model Prediction: tensor([[ 0, 10, 10, 10, 3, 2, 5, 7, 9, 6]])\nExample Untrained Model Prediction: tensor([[ 0, 4, 3, 6, 10, 10, 2, 6, 2, 2]])\nExample Untrained Model Prediction: tensor([[ 0, 9, 0, 1, 5, 10, 1, 5, 10, 6]])\nExample Untrained Model Prediction: tensor([[ 0, 1, 5, 1, 10, 1, 10, 10, 10, 10]])\nExample Untrained Model Prediction: tensor([[ 0, 1, 10, 9, 9, 9, 9, 9, 1, 5]])\nExample Untrained Model Prediction: tensor([[ 0, 3, 1, 5, 10, 10, 10, 10, 10, 10]])\nExample Untrained Model Prediction: tensor([[ 0, 3, 5, 10, 5, 10, 4, 2, 4, 2]])\nExample Untrained Model Prediction: tensor([[0, 5, 6, 2, 5, 6, 2, 6, 2, 2]])\nThis section describes the training regime for our models.\nWe stop for a quick interlude to introduce some of the tools needed to train a standard encoder decoder model. First we define a batch object that holds the src and target sentences for training, as well as constructing the masks.\nclass Batch:\n\"\"\"Object for holding a batch of data with mask during training.\"\"\"\ndef __init__(self, src, tgt=None, pad=2): # 2 = <blank>\nself.src = src\nself.src_mask = (src != pad).unsqueeze(-2)\nif tgt is not None:\nself.tgt = tgt[:, :-1]\nself.tgt_y = tgt[:, 1:]\nself.tgt_mask = self.make_std_mask(self.tgt, pad)\nself.ntokens = (self.tgt_y != pad).data.sum()\n@staticmethod\ndef make_std_mask(tgt, pad):\n\"Create a mask to hide padding and future words.\"\ntgt_mask = (tgt != pad).unsqueeze(-2)\ntgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\ntgt_mask.data\n)\nreturn tgt_mask\nNext we create a generic training and scoring function to keep track of loss. We pass in a generic loss compute function that also handles parameter updates.\nclass TrainState:\n\"\"\"Track number of steps, examples, and tokens processed\"\"\"\nstep: int = 0 # Steps in the current epoch\naccum_step: int = 0 # Number of gradient accumulation steps\nsamples: int = 0 # total # of examples used\ntokens: int = 0 # total # of tokens processed\ndef run_epoch(\ndata_iter,\nmodel,\nloss_compute,\noptimizer,\nscheduler,\nmode=\"train\",\naccum_iter=1,\ntrain_state=TrainState(),\n):\n\"\"\"Train a single epoch\"\"\"\nstart = time.time()\ntotal_tokens = 0\ntotal_loss = 0\ntokens = 0\nn_accum = 0\nfor i, batch in enumerate(data_iter):\nout = model.forward(\nbatch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n)\nloss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n# loss_node = loss_node / accum_iter\nif mode == \"train\" or mode == \"train+log\":\nloss_node.backward()\ntrain_state.step += 1\ntrain_state.samples += batch.src.shape[0]\ntrain_state.tokens += batch.ntokens\nif i % accum_iter == 0:\noptimizer.step()\noptimizer.zero_grad(set_to_none=True)\nn_accum += 1\ntrain_state.accum_step += 1\nscheduler.step()\ntotal_loss += loss\ntotal_tokens += batch.ntokens\ntokens += batch.ntokens\nif i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\nlr = optimizer.param_groups[0][\"lr\"]\nelapsed = time.time() - start\nprint(\n(\n\"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n+ \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n)\n% (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n)\nstart = time.time()\ntokens = 0\ndel loss\ndel loss_node\nreturn total_loss / total_tokens, train_state\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.\nSentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\nWe used the Adam optimizer (cite) with \\beta_1=0.9, \\beta_2=0.98 and \\epsilon=10^{-9}. We varied the learning rate over the course of training, according to the formula:\nlrate = d_{\\text{model}}^{-0.5} \\cdot \\min({step\\_num}^{-0.5}, {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\nThis corresponds to increasing the learning rate linearly for the first warmup\\_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup\\_steps=4000.\nNote: This part is very important. Need to train with this setup of the model.\nExample of the curves of this model for different model sizes and for optimization hyperparameters.\ndef rate(step, model_size, factor, warmup):\n\"\"\"\nwe have to default the step to 1 for LambdaLR function\nto avoid zero raising to negative power.\n\"\"\"\nif step == 0:\nstep = 1\nreturn factor * (\nmodel_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n)\ndef example_learning_schedule():\nopts = [\n[512, 1, 4000], # example 1\n[512, 1, 8000], # example 2\n[256, 1, 4000], # example 3\n]\ndummy_model = torch.nn.Linear(1, 1)\nlearning_rates = []\n# we have 3 examples in opts list.\nfor idx, example in enumerate(opts):\n# run 20000 epoch for each example\noptimizer = torch.optim.Adam(\ndummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n)\nlr_scheduler = LambdaLR(\noptimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n)\ntmp = []\n# take 20K dummy training steps, save the learning rate at each step\nfor step in range(20000):\ntmp.append(optimizer.param_groups[0][\"lr\"])\noptimizer.step()\nlr_scheduler.step()\nlearning_rates.append(tmp)\nlearning_rates = torch.tensor(learning_rates)\n# Enable altair to handle more than 5000 rows\nalt.data_transformers.disable_max_rows()\nopts_data = pd.concat(\n[\npd.DataFrame(\n{\n\"Learning Rate\": learning_rates[warmup_idx, :],\n\"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\nwarmup_idx\n],\n\"step\": range(20000),\n}\n)\nfor warmup_idx in [0, 1, 2]\n]\n)\nreturn (\nalt.Chart(opts_data)\n.mark_line()\n.properties(width=600)\n.encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n.interactive()\n)\nexample_learning_schedule()\nDuring training, we employed label smoothing of value \\epsilon_{ls}=0.1 (cite). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\nWe implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has\nconfidence\nof the correct word and the rest of thesmoothing\nmass distributed throughout the vocabulary.\nclass LabelSmoothing(nn.Module):\n\"Implement label smoothing.\"\ndef __init__(self, size, padding_idx, smoothing=0.0):\nsuper(LabelSmoothing, self).__init__()\nself.criterion = nn.KLDivLoss(reduction=\"sum\")\nself.padding_idx = padding_idx\nself.confidence = 1.0 - smoothing\nself.smoothing = smoothing\nself.size = size\nself.true_dist = None\ndef forward(self, x, target):\nassert x.size(1) == self.size\ntrue_dist = x.data.clone()\ntrue_dist.fill_(self.smoothing / (self.size - 2))\ntrue_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\ntrue_dist[:, self.padding_idx] = 0\nmask = torch.nonzero(target.data == self.padding_idx)\nif mask.dim() > 0:\ntrue_dist.index_fill_(0, mask.squeeze(), 0.0)\nself.true_dist = true_dist\nreturn self.criterion(x, true_dist.clone().detach())\nHere we can see an example of how the mass is distributed to the words based on confidence.\n# Example of label smoothing.\ndef example_label_smoothing():\ncrit = LabelSmoothing(5, 0, 0.4)\npredict = torch.FloatTensor(\n[\n[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0],\n]\n)\ncrit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\nLS_data = pd.concat(\n[\npd.DataFrame(\n{\n\"target distribution\": crit.true_dist[x, y].flatten(),\n\"columns\": y,\n\"rows\": x,\n}\n)\nfor y in range(5)\nfor x in range(5)\n]\n)\nreturn (\nalt.Chart(LS_data)\n.mark_rect(color=\"Blue\", opacity=1)\n.properties(height=200, width=200)\n.encode(\nalt.X(\"columns:O\", title=None),\nalt.Y(\"rows:O\", title=None),\nalt.Color(\n\"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n),\n)\n.interactive()\n)\nshow_example(example_label_smoothing)\nLabel smoothing actually starts to penalize the model if it gets very confident about a given choice.\ndef loss(x, crit):\nd = x + 3 * 1\npredict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\nreturn crit(predict.log(), torch.LongTensor([1])).data\ndef penalization_visualization():\ncrit = LabelSmoothing(5, 0, 0.1)\nloss_data = pd.DataFrame(\n{\n\"Loss\": [loss(x, crit) for x in range(1, 100)],\n\"Steps\": list(range(99)),\n}\n).astype(\"float\")\nreturn (\nalt.Chart(loss_data)\n.mark_line()\n.properties(width=350)\n.encode(\nx=\"Steps\",\ny=\"Loss\",\n)\n.interactive()\n)\nshow_example(penalization_visualization)\nWe can begin by trying out a simple copy-task. Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols.\ndef data_gen(V, batch_size, nbatches):\n\"Generate random data for a src-tgt copy task.\"\nfor i in range(nbatches):\ndata = torch.randint(1, V, size=(batch_size, 10))\ndata[:, 0] = 1\nsrc = data.requires_grad_(False).clone().detach()\ntgt = data.requires_grad_(False).clone().detach()\nyield Batch(src, tgt, 0)\nclass SimpleLossCompute:\n\"A simple loss compute and train function.\"\ndef __init__(self, generator, criterion):\nself.generator = generator\nself.criterion = criterion\ndef __call__(self, x, y, norm):\nx = self.generator(x)\nsloss = (\nself.criterion(\nx.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n)\n/ norm\n)\nreturn sloss.data * norm, sloss\nThis code predicts a translation using greedy decoding for simplicity.\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\nmemory = model.encode(src, src_mask)\nys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\nfor i in range(max_len - 1):\nout = model.decode(\nmemory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n)\nprob = model.generator(out[:, -1])\n_, next_word = torch.max(prob, dim=1)\nnext_word = next_word.data[0]\nys = torch.cat(\n[ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n)\nreturn ys\n# Train the simple copy task.\ndef example_simple_model():\nV = 11\ncriterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\nmodel = make_model(V, V, N=2)\noptimizer = torch.optim.Adam(\nmodel.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n)\nlr_scheduler = LambdaLR(\noptimizer=optimizer,\nlr_lambda=lambda step: rate(\nstep, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n),\n)\nbatch_size = 80\nfor epoch in range(20):\nmodel.train()\nrun_epoch(\ndata_gen(V, batch_size, 20),\nmodel,\nSimpleLossCompute(model.generator, criterion),\noptimizer,\nlr_scheduler,\nmode=\"train\",\n)\nmodel.eval()\nrun_epoch(\ndata_gen(V, batch_size, 5),\nmodel,\nSimpleLossCompute(model.generator, criterion),\nDummyOptimizer(),\nDummyScheduler(),\nmode=\"eval\",\n)[0]\nmodel.eval()\nsrc = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\nmax_len = src.shape[1]\nsrc_mask = torch.ones(1, 1, max_len)\nprint(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))\n# execute_example(example_simple_model)\nNow we consider a real-world example using the Multi30k German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. We also show how to use multi-gpu processing to make it really fast.\nWe will load the dataset using torchtext and spacy for tokenization.\n# Load spacy tokenizer models, download them if they haven't been\n# downloaded already\ndef load_tokenizers():\ntry:\nspacy_de = spacy.load(\"de_core_news_sm\")\nexcept IOError:\nos.system(\"python -m spacy download de_core_news_sm\")\nspacy_de = spacy.load(\"de_core_news_sm\")\ntry:\nspacy_en = spacy.load(\"en_core_web_sm\")\nexcept IOError:\nos.system(\"python -m spacy download en_core_web_sm\")\nspacy_en = spacy.load(\"en_core_web_sm\")\nreturn spacy_de, spacy_en\ndef tokenize(text, tokenizer):\nreturn [tok.text for tok in tokenizer.tokenizer(text)]\ndef yield_tokens(data_iter, tokenizer, index):\nfor from_to_tuple in data_iter:\nyield tokenizer(from_to_tuple[index])\ndef build_vocabulary(spacy_de, spacy_en):\ndef tokenize_de(text):\nreturn tokenize(text, spacy_de)\ndef tokenize_en(text):\nreturn tokenize(text, spacy_en)\nprint(\"Building German Vocabulary ...\")\ntrain, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\nvocab_src = build_vocab_from_iterator(\nyield_tokens(train + val + test, tokenize_de, index=0),\nmin_freq=2,\nspecials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n)\nprint(\"Building English Vocabulary ...\")\ntrain, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\nvocab_tgt = build_vocab_from_iterator(\nyield_tokens(train + val + test, tokenize_en, index=1),\nmin_freq=2,\nspecials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n)\nvocab_src.set_default_index(vocab_src[\"<unk>\"])\nvocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\nreturn vocab_src, vocab_tgt\ndef load_vocab(spacy_de, spacy_en):\nif not exists(\"vocab.pt\"):\nvocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\ntorch.save((vocab_src, vocab_tgt), \"vocab.pt\")\nelse:\nvocab_src, vocab_tgt = torch.load(\"vocab.pt\")\nprint(\"Finished.\\nVocabulary sizes:\")\nprint(len(vocab_src))\nprint(len(vocab_tgt))\nreturn vocab_src, vocab_tgt\nif is_interactive_notebook():\n# global variables used later in the script\nspacy_de, spacy_en = show_example(load_tokenizers)\nvocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])\nFinished.\nVocabulary sizes:\n59981\n36745\nBatching matters a ton for speed. We want to have very evenly divided batches, with absolutely minimal padding. To do this we have to hack a bit around the default torchtext batching. This code patches their default batching to make sure we search over enough sentences to find tight batches.\ndef collate_batch(\nbatch,\nsrc_pipeline,\ntgt_pipeline,\nsrc_vocab,\ntgt_vocab,\ndevice,\nmax_padding=128,\npad_id=2,\n):\nbs_id = torch.tensor([0], device=device) # <s> token id\neos_id = torch.tensor([1], device=device) # </s> token id\nsrc_list, tgt_list = [], []\nfor (_src, _tgt) in batch:\nprocessed_src = torch.cat(\n[\nbs_id,\ntorch.tensor(\nsrc_vocab(src_pipeline(_src)),\ndtype=torch.int64,\ndevice=device,\n),\neos_id,\n],\n0,\n)\nprocessed_tgt = torch.cat(\n[\nbs_id,\ntorch.tensor(\ntgt_vocab(tgt_pipeline(_tgt)),\ndtype=torch.int64,\ndevice=device,\n),\neos_id,\n],\n0,\n)\nsrc_list.append(\n# warning - overwrites values for negative values of padding - len\npad(\nprocessed_src,\n(\n0,\nmax_padding - len(processed_src),\n),\nvalue=pad_id,\n)\n)\ntgt_list.append(\npad(\nprocessed_tgt,\n(0, max_padding - len(processed_tgt)),\nvalue=pad_id,\n)\n)\nsrc = torch.stack(src_list)\ntgt = torch.stack(tgt_list)\nreturn (src, tgt)\ndef create_dataloaders(\ndevice,\nvocab_src,\nvocab_tgt,\nspacy_de,\nspacy_en,\nbatch_size=12000,\nmax_padding=128,\nis_distributed=True,\n):\n# def create_dataloaders(batch_size=12000):\ndef tokenize_de(text):\nreturn tokenize(text, spacy_de)\ndef tokenize_en(text):\nreturn tokenize(text, spacy_en)\ndef collate_fn(batch):\nreturn collate_batch(\nbatch,\ntokenize_de,\ntokenize_en,\nvocab_src,\nvocab_tgt,\ndevice,\nmax_padding=max_padding,\npad_id=vocab_src.get_stoi()[\"<blank>\"],\n)\ntrain_iter, valid_iter, test_iter = datasets.Multi30k(\nlanguage_pair=(\"de\", \"en\")\n)\ntrain_iter_map = to_map_style_dataset(\ntrain_iter\n) # DistributedSampler needs a dataset len()\ntrain_sampler = (\nDistributedSampler(train_iter_map) if is_distributed else None\n)\nvalid_iter_map = to_map_style_dataset(valid_iter)\nvalid_sampler = (\nDistributedSampler(valid_iter_map) if is_distributed else None\n)\ntrain_dataloader = DataLoader(\ntrain_iter_map,\nbatch_size=batch_size,\nshuffle=(train_sampler is None),\nsampler=train_sampler,\ncollate_fn=collate_fn,\n)\nvalid_dataloader = DataLoader(\nvalid_iter_map,\nbatch_size=batch_size,\nshuffle=(valid_sampler is None),\nsampler=valid_sampler,\ncollate_fn=collate_fn,\n)\nreturn train_dataloader, valid_dataloader\ndef train_worker(\ngpu,\nngpus_per_node,\nvocab_src,\nvocab_tgt,\nspacy_de,\nspacy_en,\nconfig,\nis_distributed=False,\n):\nprint(f\"Train worker process using GPU: {gpu} for training\", flush=True)\ntorch.cuda.set_device(gpu)\npad_idx = vocab_tgt[\"<blank>\"]\nd_model = 512\nmodel = make_model(len(vocab_src), len(vocab_tgt), N=6)\nmodel.cuda(gpu)\nmodule = model\nis_main_process = True\nif is_distributed:\ndist.init_process_group(\n\"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n)\nmodel = DDP(model, device_ids=[gpu])\nmodule = model.module\nis_main_process = gpu == 0\ncriterion = LabelSmoothing(\nsize=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n)\ncriterion.cuda(gpu)\ntrain_dataloader, valid_dataloader = create_dataloaders(\ngpu,\nvocab_src,\nvocab_tgt,\nspacy_de,\nspacy_en,\nbatch_size=config[\"batch_size\"] // ngpus_per_node,\nmax_padding=config[\"max_padding\"],\nis_distributed=is_distributed,\n)\noptimizer = torch.optim.Adam(\nmodel.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n)\nlr_scheduler = LambdaLR(\noptimizer=optimizer,\nlr_lambda=lambda step: rate(\nstep, d_model, factor=1, warmup=config[\"warmup\"]\n),\n)\ntrain_state = TrainState()\nfor epoch in range(config[\"num_epochs\"]):\nif is_distributed:\ntrain_dataloader.sampler.set_epoch(epoch)\nvalid_dataloader.sampler.set_epoch(epoch)\nmodel.train()\nprint(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n_, train_state = run_epoch(\n(Batch(b[0], b[1], pad_idx) for b in train_dataloader),\nmodel,\nSimpleLossCompute(module.generator, criterion),\noptimizer,\nlr_scheduler,\nmode=\"train+log\",\naccum_iter=config[\"accum_iter\"],\ntrain_state=train_state,\n)\nGPUtil.showUtilization()\nif is_main_process:\nfile_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\ntorch.save(module.state_dict(), file_path)\ntorch.cuda.empty_cache()\nprint(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\nmodel.eval()\nsloss = run_epoch(\n(Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\nmodel,\nSimpleLossCompute(module.generator, criterion),\nDummyOptimizer(),\nDummyScheduler(),\nmode=\"eval\",\n)\nprint(sloss)\ntorch.cuda.empty_cache()\nif is_main_process:\nfile_path = \"%sfinal.pt\" % config[\"file_prefix\"]\ntorch.save(module.state_dict(), file_path)\ndef train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\nfrom the_annotated_transformer import train_worker\nngpus = torch.cuda.device_count()\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"12356\"\nprint(f\"Number of GPUs detected: {ngpus}\")\nprint(\"Spawning training processes ...\")\nmp.spawn(\ntrain_worker,\nnprocs=ngpus,\nargs=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n)\ndef train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\nif config[\"distributed\"]:\ntrain_distributed_model(\nvocab_src, vocab_tgt, spacy_de, spacy_en, config\n)\nelse:\ntrain_worker(\n0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False\n)\ndef load_trained_model():\nconfig = {\n\"batch_size\": 32,\n\"distributed\": False,\n\"num_epochs\": 8,\n\"accum_iter\": 10,\n\"base_lr\": 1.0,\n\"max_padding\": 72,\n\"warmup\": 3000,\n\"file_prefix\": \"multi30k_model_\",\n}\nmodel_path = \"multi30k_model_final.pt\"\nif not exists(model_path):\ntrain_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\nmodel = make_model(len(vocab_src), len(vocab_tgt), N=6)\nmodel.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\nreturn model\nif is_interactive_notebook():\nmodel = load_trained_model()\nOnce trained we can decode the model to produce a set of translations. Here we simply translate the first sentence in the validation set. This dataset is pretty small so the translations with greedy search are reasonably accurate.\nSo this mostly covers the transformer model itself. There are four aspects that we didn\u2019t cover explicitly. We also have all these additional features implemented in OpenNMT-py.\n- BPE/ Word-piece: We can use a library to first preprocess the data into subword units. See Rico Sennrich\u2019s subword-nmt implementation. These models will transform the training data to look like this:\n\u2581Die \u2581Protokoll datei \u2581kann \u2581 heimlich \u2581per \u2581E - Mail \u2581oder \u2581FTP \u2581an \u2581einen \u2581bestimmte n \u2581Empf\u00e4nger \u2581gesendet \u2581werden .\n- Shared Embeddings: When using BPE with shared vocabulary we can share the same weight vectors between the source / target / generator. See the (cite) for details. To add this to the model simply do this:\nif False:\nmodel.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight\nmodel.generator.lut.weight = model.tgt_embed[0].lut.weight\n- Beam Search: This is a bit too complicated to cover here. See the OpenNMT-py for a pytorch implementation.\n- Model Averaging: The paper averages the last k checkpoints to create an ensembling effect. We can do this after the fact if we have a bunch of models:\ndef average(model, models):\n\"Average models into model\"\nfor ps in zip(*[m.params() for m in [model] + models]):\nps[0].copy_(torch.sum(*ps[1:]) / len(ps[1:]))\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\nWith the addtional extensions in the last section, the OpenNMT-py replication gets to 26.9 on EN-DE WMT. Here I have loaded in those parameters to our reimplemenation.\n# Load data and model for output checks\ndef check_outputs(\nvalid_dataloader,\nmodel,\nvocab_src,\nvocab_tgt,\nn_examples=15,\npad_idx=2,\neos_string=\"</s>\",\n):\nresults = [()] * n_examples\nfor idx in range(n_examples):\nprint(\"\\nExample %d ========\\n\" % idx)\nb = next(iter(valid_dataloader))\nrb = Batch(b[0], b[1], pad_idx)\ngreedy_decode(model, rb.src, rb.src_mask, 64, 0)[0]\nsrc_tokens = [\nvocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx\n]\ntgt_tokens = [\nvocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx\n]\nprint(\n\"Source Text (Input) : \"\n+ \" \".join(src_tokens).replace(\"\\n\", \"\")\n)\nprint(\n\"Target Text (Ground Truth) : \"\n+ \" \".join(tgt_tokens).replace(\"\\n\", \"\")\n)\nmodel_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]\nmodel_txt = (\n\" \".join(\n[vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n).split(eos_string, 1)[0]\n+ eos_string\n)\nprint(\"Model Output : \" + model_txt.replace(\"\\n\", \"\"))\nresults[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\nreturn results\ndef run_model_example(n_examples=5):\nglobal vocab_src, vocab_tgt, spacy_de, spacy_en\nprint(\"Preparing Data ...\")\n_, valid_dataloader = create_dataloaders(\ntorch.device(\"cpu\"),\nvocab_src,\nvocab_tgt,\nspacy_de,\nspacy_en,\nbatch_size=1,\nis_distributed=False,\n)\nprint(\"Loading Trained Model ...\")\nmodel = make_model(len(vocab_src), len(vocab_tgt), N=6)\nmodel.load_state_dict(\ntorch.load(\"multi30k_model_final.pt\", map_location=torch.device(\"cpu\"))\n)\nprint(\"Checking Model Outputs:\")\nexample_data = check_outputs(\nvalid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples\n)\nreturn model, example_data\n# execute_example(run_model_example)\nEven with a greedy decoder the translation looks pretty good. We can further visualize it to see what is happening at each layer of the attention\ndef mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n\"convert a dense matrix to a data frame with row and column indices\"\nreturn pd.DataFrame(\n[\n(\nr,\nc,\nfloat(m[r, c]),\n\"%.3d %s\"\n% (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n\"%.3d %s\"\n% (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n)\nfor r in range(m.shape[0])\nfor c in range(m.shape[1])\nif r < max_row and c < max_col\n],\n# if float(m[r,c]) != 0 and r < max_row and c < max_col],\ncolumns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n)\ndef attn_map(attn, layer, head, row_tokens, col_tokens, max_dim=30):\ndf = mtx2df(\nattn[0, head].data,\nmax_dim,\nmax_dim,\nrow_tokens,\ncol_tokens,\n)\nreturn (\nalt.Chart(data=df)\n.mark_rect()\n.encode(\nx=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\ny=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\ncolor=\"value\",\ntooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n)\n.properties(height=400, width=400)\n.interactive()\n)\ndef get_encoder(model, layer):\nreturn model.encoder.layers[layer].self_attn.attn\ndef get_decoder_self(model, layer):\nreturn model.decoder.layers[layer].self_attn.attn\ndef get_decoder_src(model, layer):\nreturn model.decoder.layers[layer].src_attn.attn\ndef visualize_layer(model, layer, getter_fn, ntokens, row_tokens, col_tokens):\n# ntokens = last_example[0].ntokens\nattn = getter_fn(model, layer)\nn_heads = attn.shape[1]\ncharts = [\nattn_map(\nattn,\n0,\nh,\nrow_tokens=row_tokens,\ncol_tokens=col_tokens,\nmax_dim=ntokens,\n)\nfor h in range(n_heads)\n]\nassert n_heads == 8\nreturn alt.vconcat(\ncharts[0]\n# | charts[1]\n| charts[2]\n# | charts[3]\n| charts[4]\n# | charts[5]\n| charts[6]\n# | charts[7]\n# layer + 1 due to 0-indexing\n).properties(title=\"Layer %d\" % (layer + 1))\ndef viz_encoder_self():\nmodel, example_data = run_model_example(n_examples=1)\nexample = example_data[\nlen(example_data) - 1\n] # batch object for the final example\nlayer_viz = [\nvisualize_layer(\nmodel, layer, get_encoder, len(example[1]), example[1], example[1]\n)\nfor layer in range(6)\n]\nreturn alt.hconcat(\nlayer_viz[0]\n# & layer_viz[1]\n& layer_viz[2]\n# & layer_viz[3]\n& layer_viz[4]\n# & layer_viz[5]\n)\nshow_example(viz_encoder_self)\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\nExample 0 ========\nSource Text (Input) : <s> Zwei Frauen in pinkfarbenen T-Shirts und <unk> unterhalten sich vor einem <unk> . </s>\nTarget Text (Ground Truth) : <s> Two women wearing pink T - shirts and blue jeans converse outside clothing store . </s>\nModel Output : <s> Two women in pink shirts and face are talking in front of a <unk> . </s>\ndef viz_decoder_self():\nmodel, example_data = run_model_example(n_examples=1)\nexample = example_data[len(example_data) - 1]\nlayer_viz = [\nvisualize_layer(\nmodel,\nlayer,\nget_decoder_self,\nlen(example[1]),\nexample[1],\nexample[1],\n)\nfor layer in range(6)\n]\nreturn alt.hconcat(\nlayer_viz[0]\n& layer_viz[1]\n& layer_viz[2]\n& layer_viz[3]\n& layer_viz[4]\n& layer_viz[5]\n)\nshow_example(viz_decoder_self)\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\nExample 0 ========\nSource Text (Input) : <s> Eine Gruppe von M\u00e4nnern in Kost\u00fcmen spielt Musik . </s>\nTarget Text (Ground Truth) : <s> A group of men in costume play music . </s>\nModel Output : <s> A group of men in costumes playing music . </s>\ndef viz_decoder_src():\nmodel, example_data = run_model_example(n_examples=1)\nexample = example_data[len(example_data) - 1]\nlayer_viz = [\nvisualize_layer(\nmodel,\nlayer,\nget_decoder_src,\nmax(len(example[1]), len(example[2])),\nexample[1],\nexample[2],\n)\nfor layer in range(6)\n]\nreturn alt.hconcat(\nlayer_viz[0]\n& layer_viz[1]\n& layer_viz[2]\n& layer_viz[3]\n& layer_viz[4]\n& layer_viz[5]\n)\nshow_example(viz_decoder_src)\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\nExample 0 ========\nSource Text (Input) : <s> Ein kleiner Junge verwendet einen Bohrer , um ein Loch in ein Holzst\u00fcck zu machen . </s>\nTarget Text (Ground Truth) : <s> A little boy using a drill to make a hole in a piece of wood . </s>\nModel Output : <s> A little boy uses a machine to be working in a hole in a log . </s>\nHopefully this code is useful for future research. Please reach out if you have any issues.\nCheers, Sasha Rush, Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, Stella Biderman",
    "annotations": [],
    "raw_file_hash": null,
    "asset_paths": [],
    "processing_status": "pending",
    "error_message": null,
    "obsidian_path": null,
    "tags": [
      "TODO"
    ],
    "metadata": {
      "raindrop_id": 899095328,
      "collection_id": 49346813,
      "cover": "",
      "excerpt": ""
    }
  },
  {
    "id": "15b25998-b6a9-4924-aae8-33920a0f404f",
    "source_type": "article",
    "source_url": "https://graphcore-research.github.io/posts/gemma/",
    "source_file_path": null,
    "title": "A transformer walk-through, with Gemma",
    "authors": [
      "Unknown"
    ],
    "created_at": "2024-11-13T07:44:10.274000+00:00",
    "ingested_at": "2026-01-01T20:24:02.369758",
    "full_text": "A transformer walk-through, with Gemma\nTransformer-based LLMs seem mysterious, but they don\u2019t need to. In this post, we\u2019ll walk through a modern transformer LLM, Google\u2019s Gemma, providing bare-bones PyTorch code and some intuition for why each step is there. If you\u2019re a programmer and casual ML enthusiast, this is written for you.\nOur problem is single-step prediction: take a string e.g. \u201cI want to move\u201d, and use an already-trained language model (LM) to predict what could come next. This is the core component of chatbots, coding assistants, etc, since we can easily chain these predictions together to generate long strings.\nWe\u2019ll walk through this example using Gemma 2B, with an accompanying notebook (github, Colab) which provides unadorned code for everything we\u2019ll see.\nThere are two ways to read this post: the main text describes the implementation, or what is required to run Gemma. The expandable sections add details about why the code does this, with some of the machine learning intuition behind the code.\nUnderstanding preliminaries (dot product)\nTo follow some of the optional \u201cunderstanding\u201d sections in this post, it\u2019s worth recapping the dot product between two vectors:\ndef dot(a, b): return sum(a_i * b_i for a_i, b_i in zip(a, b))\nIt acts as a test of agreement. Here\u2019s an example of the dot product between some vectors in 2D:\nFrom this, we see that longer lines have larger magnitude dot products, and for a given length of vectors, the dot product will be largest (most positive) when they are in the same direction, zero when perpendicular, and smallest (most negative) when they are in the opposite direction.\nDot products between 2-vectors are quite easy to visualise. But Gemma involves dot products between 2048-, 256- and 16384-vectors, which are a bit more challenging! However, we can still think of these high-dimensional dot products as rating the agreement between two vectors.\nTokenization\nTokenize the input string, splitting & mapping it to token IDs.\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"google/gemma-2b\")\ninput_ids = tokenizer(\"I want to move\").input_ids\n# input_ids = [2, 235285, 1938, 577, 3124]\nThe model works on sequences of subword tokens, represented as integers in the range [0, 256000), such as 1938 (=> \u201c\u2581want\u201d). Since the input comes in as a single long string, we use a tokenizer to first split the string into tokens, second to map these tokens to numeric IDs.\nUnderstanding tokenisation\nTokenisation is defined via a vocabulary, a large set of tokens that are understood by the model. These tokens might be whole words e.g. \u201cThe\u201d or parts of words, e.g. \u201c superlative\u201d is split into [\u201c\u2581super\u201d, \u201clative\u201d].\ninput_ids = tokenizer(\"The superlative\").input_ids\nprint(tokenizer.convert_ids_to_tokens(input_ids))\n# => ['<bos>', 'The', '\u2581super', 'lative']\nThis illustrates a few interesting things about the tokenizer.\n- It is subword, able to split long words. (Since most characters are included, it can tokenize even gibberish \u201cnvdkjsv\u201d => [\u201cn\u201d, \u201cvd\u201d, \u201ck\u201d, \u201cjs\u201d, \u201cv\u201d].)\n- It doesn\u2019t throw whitespace away, mapping \u201c \u201c to \u201c\u2581\u201d.\n- It doesn\u2019t normalise capitalisation (651 => \u201cThe\u201d, 1175 => \u201cthe\u201d).\n- It prepends a special \u201cbeginning of sequence\u201d token, 2 => \u201c<bos>\u201d.\n- The vocabulary is very large,\nlen(tokenizer.vocab)\nis 256000.\nAll but the last point are standard for modern LLMs. We use subword tokenization since it gives great coverage of any input text. We include whitespace and don\u2019t normalise capitalisation to avoid all the cases where these rules have exceptions and ambiguities, passing these down to the model to figure out (e.g. when is the token \u201capple\u201d the same thing as \u201cApple\u201d?)\nWith subword tokenization, vocabulary size is a free choice of the model designer. Many modern models use vocabularies in the 30-60k range, while Gemma\u2019s is much larger at 256k. Large vocabularies can help coverage of the long tail of language, and there is a mild efficiency trade-off between larger vocabularies and shorter sequences versus smaller vocabularies and longer sequences.\nSince this article aims at understanding the algorithm, I must apologise at this point. We\u2019re just going to treat subword tokenization as a black box. If you want to learn more, Gemma\u2019s tokenizer uses byte pair encoding (BPE), and at inference time the problem is a search through the vocabulary for a sequence of tokens that match the input string (there are generally multiple possible tokenizations of a given input).\nEmbedding lookup\nConvert token IDs to 2048-element vectors by looking them up in an embedding table.\nhiddens = p.embedding[input_ids]\n# p.embedding.shape = (256000, 2048)\n# input_ids.shape = (5,)\n# hiddens.shape = (5, 2048)\nFor each input token ID (e.g. 1938 => \u201c\u2581want\u201d), select the corresponding row from the large embedding table. The result, which we\u2019ll call hiddens\n, has shape (5, 2048), where there are 5 tokens along the sequence dimension and 2048 components along the hidden dimension.\nUnderstanding embedding\nAs we\u2019re about to see, most operations in Gemma are a dot product similarity between high-dimensional vectors. On one level, embedding lookup is just the simplest way to get from IDs to high-dimensional vectors. Note also that the numeric IDs associated with tokens are arbitrary (5000 => \u201c\u753b\u50cf\u201d, 5001 => \u201c\u2581fish\u201d, 5002 => \u201cmento\u201d); an embedding table is desirable as it has no sense of locality or smoothness between consecutive IDs.\nSince the model attempts to predict words based on similarity, we\u2019d expect words that appear in the same contexts to have similar embeddings. In the figure below, we plot the cosine similarity between a few selected tokens from the vocabulary. Cosine similarity is the dot product with the length of input vectors normalised out, calculated as dot(a, b) / sqrt(dot(a, a) * dot(b, b))\n.\nWords with similar functions in language (colours, place names, adjectives) have similar embeddings. It\u2019d be fun to read something into \u201cTokyo\u201d and \u201cLondon\u201d being closer than \u201cParis\u201d to \u201cpleasant\u201d and \u201cvibrant\u201d, but that is perhaps a bit risky \u2014 this embedding similarity metric is a very blunt tool.\nPost-embedding rescaling\nScale up hiddens\nby sqrt(2048)\n.\nhiddens *= hiddens.shape[-1] ** 0.5\nIn Gemma, embedding parameters are shared between the input embedding lookup and the output embedding projection. This is a reasonable design, but some care is required to give appropriate scale to inputs and outputs. The scaling factor of sqrt(hidden_size)\nmakes inputs larger without interfering with the output projection.\nUnderstanding post-embedding rescaling\nTo see the need for a scaling factor, we have to skip ahead and consider the final output projection as well as the input embedding lookup. The final projection looks like this:\nlogits = hiddens @ p.embedding.T\nSo each element of logits\ncomes from a dot product between hiddens\nand an embedding vector of length 2048. Typically, we initialise operations to preserve the scale or standard deviation of their inputs. In this case, this means the scale of embedding\nshould be 1/sqrt(2048)\n.\nhiddens = torch.randn(10, 2048)\nembedding_init = torch.randn(100, 2048) / 2048**0.5\nlogits = hiddens @ embedding_init.T\n# hiddens.std() = 1.005\n# logits.std() = 1.02\nReturning to the input embedding lookup, this rule means that hiddens\ncoming out of the embedding lookup also have scale 1/sqrt(2048)\n. This isn\u2019t an immediate problem: as we shall see, the next operation that happens is a normalisation that resets the scale. The problem occurs when we add the contribution of the first attention layer to hiddens\n. At this point, a small incoming scale would mean that hiddens\nare in-effect replaced by the output of the first attention operation.\nPost-embedding scaling therefore preserves the idea of a residual network, that the contributions of attention and MLP layers should be additive. After the first attention layer:\n# With scaling\nhiddens = embeddings + attention(embeddings)\n# Without scaling\nhiddens \u2248 attention(embeddings)\nTransformer layer\nFor each of Gemma\u2019s 18 transformer layers, add attention(norm(hiddens))\nthen mlp(norm(hiddens))\nback into hiddens\n.\nfor p_attn, p_mlp in p.layers:\nhiddens += attention(p_attn, rms_norm(p_attn.norm, hiddens))\nhiddens += mlp(p_mlp, rms_norm(p_mlp.norm, hiddens))\nHere, Gemma uses the concept of residual layers, which build deep (many-layered) functions using the iteration x = x + layer(norm(x))\nrather than the simpler MLP iteration x = layer(x)\n.\nUnderstanding residual layers\nFirst, some background: when we talk of representations, these are the 2048-element vectors that pass through Gemma\u2019s operations. The idea of a residual layer is that every layer shouldn\u2019t create an entirely new type of representation, replacing the old one with x = layer(x)\n. It should instead just tweak the existing representation with an update x = x + layer(norm(x))\n.\nIf we call x\nthe skip connection, layer(norm(x))\nthe residual connection, and use standard deviation ($\\sigma_s$, $\\sigma_r$ respectively) to give an indication of the scale of the contribution of each, we can look at each layer\u2019s residual contribution when the skip and residual are added. Since uncorrelated variances would add, we use $\\sqrt{\\sigma_r^2 / (\\sigma_r^2 + \\sigma_s^2)}$ as a metric for the update scale of the residual. If 0, the output is all from the skip connection, the layer doesn\u2019t contribute much at all; if 1, the output is all from the layer, the skip connection doesn\u2019t contribute much.\nFor our short example, this shows that most layers make a very small contribution, tweaking the existing representation rather than replacing it. A few layers make a larger contribution, notably the first attention layer and the last MLP layer.\nRMSNorm\nDivide out the root-mean-squared \u201cRMS\u201d from each token to reset it to 1. Then multiply each element by a learned scaling parameter.\ndef rms_norm(x: Tensor) -> Tensor:\n# x.shape = (5, 2048)\nz = x.to(torch.float32, copy=True)\nz /= torch.sqrt((z ** 2).mean(-1, keepdim=True) + p.eps)\nz *= (1 + p.weight.float())\n# p.weight.shape = (2048,)\nreturn z.to(x.dtype)\n# return.shape = (5, 2048)\nz = rms_norm(hiddens)\nIt\u2019s worth considering shapes and data types carefully. If the input shape x.shape\nis (5, 2048), then the normaliser torch.sqrt(...)\nhas shape (5, 1). This is because we\u2019re doing a mean over the last axis, processing each token in the sequence independently. The division z /= torch.sqrt(...)\nperforms broadcasting, dividing each element of a 2048-vector by the same value (different for each token).\nSince some data types have limited range (e.g. 65e3 in float16 versus 3e48 in float32), it may not be safe to do x ** 2\ndirectly. If abs(x) > 256\n, this will overflow and cause an error. Therefore, we cast to float32\nat the beginning, and back to the original x.dtype\nat the end of the op. There is also a small epsilon value added to the denominator as sqrt(... + p.eps)\n, which helps protect against division by a very small number.\nUnderstanding RMSNorm\nTo understand why Gemma has rms_norm\nat every layer, consider the update hiddens = hiddens + layer(rms_norm(hiddens))\n. The layers aren\u2019t linear, but they also don\u2019t saturate when inputs are large, so if we\u2019re just thinking about scale, we could imagine something like the following:\n# Without norm\nhiddens = 1\nfor _ in range(18):\nhiddens += 2 * hiddens # layer(x) = 2 * x\n# hiddens = 387420489\n# With norm\nhiddens = 1\nfor _ in range(18):\nhiddens += 2 # layer(norm(x)) = 2\n# hiddens = 37\nWithout a norm, the hidden representation will grow exponentially; with a norm, it grows only linearly. To test this in a more realistic setting, let\u2019s look at the scale (standard deviation) of the hidden representation before each attention and MLP layer in Gemma, both in the original model (with RMSNorm) and when all norms are replaced by the identity function (without RMSNorm).\nNote that removing the norm of an already-trained model wouldn\u2019t be allowed in any case, but this plot illustrates how normalisation helps to restrain the scale of the hidden representation across multiple layers. Exploding scale is undesirable: it can cause saturation of nonlinearities in the model, as well as numerical difficulties.\nAttention\nThe first type of residual layer in Gemma is attention. It combines representations across the sequence dimension.\ndef self_attn(q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n# t=target, s=source, n=kv-heads, m=q-heads-per-kv, d=head-dim\na = torch.einsum(\"tnmd, snd -> nmts\", q, k) / sqrt(q.shape[-1])\na += torch.full(a.shape[-2:], -torch.inf, dtype=a.dtype).tril_(-1).T\na = a.softmax(dim=-1)\nreturn torch.einsum(\"nmts, snd -> tnmd\", a, v)\ndef attention(x: Tensor) -> Tensor:\n# x.shape = (5, 2048)\nq = (x @ p.q_proj.T).unflatten(-1, (1, 8, 256))\nk = (x @ p.k_proj.T).unflatten(-1, (1, 256))\nv = (x @ p.v_proj.T).unflatten(-1, (1, 256))\n# q.shape = (5, 1, 8, 256)\n# k.shape = (5, 1, 256)\n# v.shape = (5, 1, 256)\no = self_attn(q, k, v)\n# o.shape = (5, 1, 8, 256)\nreturn o.flatten(1) @ p.o_proj.T\n# return.shape = (5, 2048)\nThe first step in attention is a projection (dot product with a parameter matrix) from the 2048-dimension input to 8 separate 256-dimension query heads, one 256-dimension key and one 256-dimension value. This projection is done independently but in parallel for each token using the dot product operator @\n. Note that PyTorch typically keeps projections in transposed layout, so we need to transpose q_proj\netc via q_proj.T\nbefore the dot product.\nThe 8 query heads are processed independently but in parallel by self_attn\n.\n- (Einsum does a dot product but allows dimensions to be reordered.) Einsum\ntnmd, snd -> nmts\nto take the first argument of shape(target, kv-heads, q-heads-per-kv, head-dim)\nand second of shape(source, kv-heads, head-dim)\nand run dot product over head-dim (256) to get a result of shape(kv-heads, q-heads-per-kv, target, source)\n. We can think of this as a(target, source)\nattention matrix per query head. - Scale the attention matrix by\n/sqrt(256)\n. - Create a causal mask using\ntorch.full(...)\n, setting all source positions that are to the right of (i.e. after) target in the(target, source)\nmatrix to-infinity\n. This prevents future tokens from influencing past ones. - Use\nsoftmax(x)\nto exponentiatez = exp(x)\nthen normalisez /= sum(z)\nto get attention weights in the range [0, 1]. This is run independently for each target, normalising to sum to 1 over all sources. The causal-masked positions are nowexp(-infinity) == 0\n. - Run a final einsum,\nnmts, snd -> tnmd\nto take the first argument of shape(kv-heads, q-heads-per-kv, target, source)\nand second of shape(source, kv-heads, head-dim)\nand run dot product over source (of size sequence length, i.e. 5), to get a result of shape(target, kv-heads, q-heads-per-kv, head-dim)\n. This is a weighted sum over the sequence, where the weights come from the(target, source)\nattention matrix.\nFinally, the outputs of the heads are mixed by flattening them from (5, 1, 8, 256) -> (5, 2048) then following a final output projection, o_proj\n.\nUnderstanding Attention\nMulti-query attention in Gemma can be thought of as 8 independent attention operations, which share some parameters (more on that later). The first step is projection by q_proj\n, k_proj\nand v_proj\n, which are trained model parameter matrices of shape (2048, 256)\n. This means picking 256 directions to test each 2048-vector in, then creating a 256-vector containing the result of each of these tests. From this, we get 5 query vectors, 5 key vectors and 5 value vectors, all of 256 dimensions.\nA high-level understanding of the next bit has an analogy to databases: there is a database of key->value mappings, and we want to retrieve them based on a query. Rather than just retrieving one (e.g. the closest match between query and key), however, we\u2019re going to get a weighted mixture of everything.\nFirst, we use dot product to test for similarity between queries and keys (step 1 above). This matches queries against keys, producing a score for each (q, k) pair. We scale the result by 1/sqrt(256)\n, which compensates for the sum inside the dot product over 256 entries.\nThe next step, causal masking, ensures that in the remaining steps, each token can only \u201csee\u201d itself and past tokens. This is essential for generative autoregressive models - if a token had to see future tokens before the next one was generated, you\u2019d immediately get into a chicken-and-egg problem.\nAttention weights: Next, softmax turns these scores into weights, which are in the range [0, 1] and always sum to 1 over all keys corresponding to a single query. The difference between the scores for two tokens going into a softmax becomes the ratio between the weights of those two tokens after the softmax. For example, if you have softmax over two tokens X and Y, with scores 0 and -0.693 respectively, you\u2019d know that Y must have 1/2 the weight of X, so the softmax would output 2/3 for X and 1/3 for Y.\nThese weights are quite fun to visualise, so I expect you\u2019ve seen them before. Here they are for our example, for every head (8 per layer) in every layer (18):\nEach square represents an attention weight matrix after the softmax. Dark pixels are high weight, and light pixels are low weight. The x-axis is the source position, corresponding to the key. The y-axis is the target position, corresponding to the query. The lower-diagonal pattern shows the causal mask \u2014 a source must have zero weight for all targets to the right of it. The leading diagonal is the score for the token\u2019s own key. The ordering of layers is not arbitrary, as layers are executed in sequence. Attention head ordering is arbitrary since heads are executed in parallel. We have therefore reordered the heads in the plot above; the \u201cflatter\u201d heads are at the top, and the \u201cspikier\u201d heads are at the bottom.\nEven from such a simple sequence of just 5 tokens, we can see some patterns. Some heads are flat, taking a broad average. Many look at the first token. Some consistently look one token to the left. However, the general point is that these attention weight matrices illustrate the power of attention. The model contains a large number of attention heads, each of which can move information between tokens flexibly, depending on position and/or content.\nMixing: These weights are then used to perform a weighted sum of the value vectors that were generated earlier. This is also just a dot product, this time over the sequence axis (5 tokens). The attention weights we looked at are important because they determine how much of the output comes from each source token in this step.\nAt this point, there are 8 sequence-mixed outputs of shape (5, 256)\n, one for each head. The final step is to mix across heads. This involves projecting up with the output projection o_proj\nso that each head produces a 2048-vector, then adding up the results over all heads. (When the heads are computed in parallel, this is equivalent to reshaping and then projecting, as per the code.)\nGrouped-query attention: Gemma includes a modification to the above scheme which is called grouped-query attention, where each head is not fully independent, but shares the key and value projection across 8 query heads. In Gemma 2B, this is all the query heads. This is why k_proj\nand v_proj\nare of shape (2048, 256)\nwhile q_proj\nis of shape (2048, 8*256)\n. This turns out to be very useful for the efficiency of token-by-token generation, which has to re-download keys and values at each generation step. But grouped-query attention doesn\u2019t change the picture much as we seek to understand attention.\nDimensions: There are a few design choices in attention:\n- Head dimension (256) - typically set in the range [64, 256]. The idea is to have enough space to express interesting interactions while leaving space for enough query heads given a fixed compute budget.\n- Number of query heads (8) - typically set so that the product of number of query heads and head dimension is equal to the hidden dimension (2048). Each head has its own attention weight matrix over the context, so increasing this number gives more flexibility to the model, at the cost of being more expensive to compute.\n- Number of query heads per key-value head (8) - a trade-off between the model performance (best with a key-value head per query head) and efficiency at inference time (best with all query heads sharing a single key-value head).\n- Value head dimension (256) - in principle, this is independent of the query-key head dimension, but it\u2019s usually set to the same value.\nRotary positional encoding (RoPE)\nWhen we described attention, we omitted one line from the definition, so we could talk about it now. After projecting to get q\nand k\n, Gemma replaces them using rotary positional encoding, which allows attention to vary depending on relative position.\ndef rotate(z: Tensor, cos: Tensor, sin: Tensor) -> Tensor:\nzx, zy = z.unflatten(-1, (2, -1)).movedim(-2, 0)\nreturn torch.cat([zx * cos - zy * sin, zy * cos + zx * sin], -1)\ndef embed_rotate(q: Tensor, k: Tensor, theta: float) -> Tensor:\n# q.shape = (5, 1, 8, 256)\n# k.shape = (5, 1, 256)\nd = q.shape[-1]\nfreq = theta ** -(torch.arange(0, d, 2, dtype=torch.float) / d)\nangle = torch.arange(q.shape[0])[:, None] * freq\ncos = angle.cos().to(q.dtype)\nsin = angle.sin().to(q.dtype)\nreturn (\nrotate(q, cos[:, None, None, :], sin[:, None, None, :]),\nrotate(k, cos[:, None, :], sin[:, None, :]),\n)\n# In attention()\nq, k = embed_rotate(q, k, theta=p.rope_theta)\nRotary positional encoding transforms q\nand k\n, keeping them the same shape. It does the same thing for every attention head, so we can think of it as mapping from (5, 256) -> (5, 256)\n. It transforms these in pairs, treating the 256-vector as 128 $\\times$ 2-vectors.\n- Generate an array of 128 different angular frequencies from 1 to\n1/theta = 1/10000\n. - For each frequency and position in the sequence, calculate the phase angle\nfreq * position\n. This is the angle (in radians) that we will rotate the corresponding 2-vector by. - Compute\ncos\nandsin\nof that angle for the rotation. - Rotate each head within\nq\nandk\nindependently. Use the first 128 components of each head as x-coordinates, and the second 128 components as y-coordinates to give the 2-vectors to be rotated. Then use trigonometryx' = x cos a - y sin a\n,y' = y cos a + x sin a\nto calculate the rotation. - Concatenate\nx'\nandy'\nback into their original position inq\nork\n.\nInserting this transformation between the calculation of q,k\nand the self_attn\nfunction that uses them to compute attention is sufficient to give Gemma the ability to deliberately attend to tokens based on position, not just content.\nUnderstanding rotary positional encoding\nLet\u2019s start with an example. Imagine we have the same q\nand k\nvectors in each position, (x, y) = (1, 0)\n. Then, after rotary encoding with two different frequencies (the highest with frequency 1, and component 20/128 with frequency 0.24), we get this:\nThe important property to note here is the rotational invariance of the dot product. Since the pre-rotation vectors were the same, the dot product $k_0 \\cdot k_1$ is the same as $k_1 \\cdot k_2$, and $k_2 \\cdot k_3$, etc. (Similarly, $k_0 \\cdot k_2 = k_1 \\cdot k_3$, etc.) So, the dot product between RoPE-encoded vectors doesn\u2019t depend on the absolute position of the tokens [0, 1, 2, 3, 4]\n, but only on the relative offset between them and the original vector before it was rotated.\nSince the dot product (einsum) between the 256-vectors q\nand k\nis identical to the sum over all 128 $\\times$ 2-vectors, the rotational invariance property of each 2-vector is carried over into the attention matrix, which means that the attention weights can only depend on the original projected q\n, k\nand the relative offset in their positions.\nDifferent frequencies: Note that rotary encoding doesn\u2019t just use a single frequency for all of the 128 $\\times$ 2-vectors. This is critical. With dot product, it\u2019s impossible to tell whether a vector has been rotated by a\nor 2*pi - a\nor 2*pi + a\nor 4*pi - a\n, etc. But even though a single component might be ambiguous, if enough different frequencies are used every position can be distinguished.\nWe may also wish to allow some heads where position matters greatly, requiring high-frequency rotations, and some other heads where position doesn\u2019t matter at all and the head just looks for certain tokens, wherever they appear in the sequence, requiring low-frequency rotations.\nIt\u2019s intuitive to look at the wavelength of rotations, calculated as 2*pi/freq\n. This is equivalent to the position offset before the rotation starts repeating itself. Here are the wavelengths of Gemma\u2019s components:\nWe see from this that Gemma\u2019s highest frequency component has a wavelength of about 6, and the lowest has about 60000. This allows the shortest wavelengths to distinguish between individual tokens, while the longest wavelengths can ignore position entirely within the maximum sequence length of 8192.\nWe can look at how long each q-vector is for every query head and rotary component, over our example text:\nSince this is averaged over just 5 tokens of sequence length, it is quite noisy \u2014 don\u2019t read too much into this, but we can see that heads vary in terms of which query components they use. Some position-agnostic heads use longer wavelength components, while position-sensitive heads use shorter wavelength components. The strong stripe near to the maximum sequence length (8192) is interesting. I\u2019m afraid I can\u2019t offer much as to why that sits there.\nMulti-layer perceptron (MLP, GeGLU)\nThe second type of residual layer in Gemma is the multi-layer perceptron (MLP), specifically the Gaussian error gated linear unit (GeGLU). This is a bit simpler than attention:\ndef mlp(x: Tensor) -> Tensor:\n# x.shape = (5, 2048)\ngate = x @ p.gate_proj.T\nup = x @ p.up_proj.T\nz = nn.functional.gelu(gate) * up\n# {gate, up, z}.shape = (5, 16384)\nreturn z @ p.down_proj.T\n# return.shape = (5, 2048)\nThe MLP takes input representations for each token as a 2048-vector and then applies the same transformation to each of the tokens independently.\nFirst, make two separate projections (dot products with trained parameter matrices) to get two 16384-vectors. One of these vectors, called the gate is passed through a nonlinear function called gelu that applies to each element. We\u2019ll treat the function as a black box, but to a first approximation, gelu(x)\nis quite close to max(0, x)\n. The gate and other vector are multiplied element by element, then another trained down-projection produces the result, a 2048-vector.\nUnderstanding the MLP\nThe MLP is a function from a 2048-vector representing a single token to a 2048-vector. The code shown above runs in parallel over the sequence axis, but unlike in attention, each token is processed independently. In this section, we\u2019ll build up the complex behaviour of Gemma\u2019s GeGLU MLP with a tiny example based on 2-vectors (and a 3-vector inside the MLP).\nNote that the acronym MLP stands for multi-layer perceptron, which is slightly old-fashioned language to talk about a chain of dot product (projections) and nonlinearities.\nA. Linear\nWe start with a simple linear projection (dot product) from input to output, specified by a random $2 \\times 2$ matrix. Each output component is a weighted sum of the inputs. The code is simply:\ny = x @ proj\n# x.shape = y.shape = (2,)\nIf we look at the first component of the output as a function of the inputs, we see:\nThis is what a linear projection always looks like \u2014 a flat slope. It\u2019s certainly possible for this function to capture some interesting properties of the data, especially when it works on 2048-vectors rather than 2-vectors. However, it will become much more powerful with a few additions.\nB. ReLU\nThe core idea of the MLP is that we wish to introduce depth into the model, a sequence of layers that transform the input in steps. However, the dot product has the property that a sequence of dot products can be reduced to a single dot product, i.e. there exists a proj_b\nsuch that (x @ proj_1) @ proj_2 == x @ proj_b\n.\nSequences of dot products need to be broken up if they\u2019re going to be any more powerful than the simple linear projection we\u2019ve already seen. The simplest way to do this is by transforming each element of the vector individually by an elementwise nonlinear function. One such function is the rectified linear unit or ReLU, relu(a) = max(0, a)\n:\nOur ReLU MLP now runs:\nz = relu(x @ gate_proj)\ny = z @ down_proj\n# x.shape = y.shape = (2,)\n# z.shape = (3,)\nSince ReLU has two distinct regimes with z == 0\nand z > 0\n, we are interested in which half we\u2019re in. If we take each component of z\nand give z > 0\na different colour (red, green, blue), we can get a map that shows which combination of z[i] > 0\nfor each input point x\n:\nIn the top right off-white segment, we have all three components of z\n\u201cactive\u201d (not saturating at zero), so in this region, we have z = x @ gate_proj\n(the ReLU disappears). In the yellow region on the left, we know the blue component z[2]\nis saturating, so in this region, we have z = (x @ gate_proj) * [1, 1, 0]\n, effectively removing that component. Within each coloured region, z\nis a linear function of x\n.\nOnce we run the down-projection, each component of y\nis a dot product between z\nand a vector of trained weights, so the result remains piecewise linear, transitioning at the boundaries we\u2019ve just seen:\nC. ReGLU\nThese piecewise linear functions are surprisingly powerful already \u2014 the pretraining procedure can manipulate the transitions as well as the slopes of each region. But we might propose more power by making each region quadratic rather than linear. This idea gives us the gated linear unit (GLU):\nz = relu(x @ gate_proj) * (x @ up_proj)\ny = z @ down_proj\nWith the same gate_proj\n, the regions in this version are the same as before; the only difference is that within each region, we have a quadratic function of x\n.\nNotice we can still have sharp edges at the region boundaries, but within each region, the function is now curved.\nD. GeGLU\nThe final change is to substitute the Gaussian error linear unit (GELU) for the ReLU. The definition isn\u2019t too important for our discussion, just that it looks like a smoother version of ReLU:\nThe idea of GELU is that it will allow us to build smoother functions than ReLU. I think this is done primarily for the sake of optimisation during training, but it might be that this gives better shapes of function for inference too.\nPlugging this into the gated linear unit, we have the full form of a GeGLU MLP as per the original code. It looks quite similar to the ReGLU, but you should be able to see that the transitions between regions are considerably smoother.\nOur approach of considering distinct regions is broken down by the GELU, which doesn\u2019t saturate at exactly zero, so does not create strict regions where components of z\ncan be discarded. However, since the GELU is quite similar to ReLU, it\u2019s still somewhat reasonable to think in terms of piecewise quadratic regions, at least at a coarse enough scale.\nSummary\nA final figure might help review the journey we\u2019ve been on, from Linear -> ReLU -> ReGLU -> GeGLU. To make things legible, we\u2019re now looking at a slice through the surfaces we\u2019ve seen so far, setting x[1]\nto a constant value, and just looking at how y[0]\ndepends on x[0]\n.\nSo Gemma\u2019s MLP, the GeGLU, can be thought of as a piecewise-quadratic function with smooth boundaries between the pieces. Where our example had 6 regions across a 2-vector input, Gemma\u2019s MLPs may have a vast number of regions (perhaps $10^{2000}$) across their 2048-vector input.\nThe purpose of the MLP in Gemma is to use this function to independently transform each token, ready to form another attention query or ready to match against output tokens. Although MLPs cannot fuse information from across the context by themselves (which is the fundamental task of a language model), our experience shows that including the MLP makes attention much more efficient at doing exactly this.\nFinal norm\nBefore we predict the output tokens, we run RMSNorm one final time on the token representations:\nhiddens = rms_norm(p.final_norm, hiddens)\nUnderstanding the final norm\nWe include a final norm because, even though the RMSNorm on the residual branches has stopped the hidden scale from exploding exponentially, it still may have grown considerably. High RMS values going into the output projection would cause very spiky (over-confident) softmax outputs, so a final norm allows the model to use large scale within the body of the transformer without propagating them to become over-confident outputs.\nFor our example:\n| Token: | <bos> | I | \u2581want | \u2581to | \u2581move |\n|---|---|---|---|---|---|\n| Before RMSNorm, RMS: | 88.5 | 16.3 | 14.4 | 12.9 | 12.6 |\n| After RMSNorm, RMS: | 2.2 | 2.2 | 2.2 | 2.2 | 2.2 |\nOutput projection\nFinally, project the representation 2048-vector up to a 256000-vector, independently but in parallel for each token in the sequence, producing a score for every possible next-token prediction.\nlogits = hiddens @ p.embedding.T\n# hiddens.shape = (5, 2048)\n# logits.shape = (5, 256000)\nThis operation shares the same parameters embedding\nused in the embedding lookup step earlier. This is just as well, as it consumes a chunky 0.5B parameters of the 2B parameter model.\nUnderstanding the output projection\nThere\u2019s not much to add beyond our earlier discussion \u201cunderstanding embedding lookup\u201d. In the same way as embedding lookup, the 2048-vector space gives enough freedom to predict 256000 tokens, since it can exploit commonalities between words, placing similar words together in embedding space.\nThe output of this hidden-embedding product is a score for each token in the vocabulary. We can turn this score into a probability using probability = softmax(score)\n, where y = softmax(x)\nmeans z = exp(x)\n, y = z / z.sum()\n. For our example, this gives the following top 10 results after each token:\nTo follow the predictions in context, read the titles from left to right to a given column, then scan down the plot for a list of possible next words. We see that in some cases the model is very confident, for example after \u201cI want\u201d, Gemma is quite confident the next token is \u201cto\u201d. Sometimes the output is flatter: after \u201cI want to\u201d, Gemma thinks there\u2019s a wide range of likely predictions.\nAt this point, it\u2019s up to the application to do what it wants with these predictions, so our journey is at an end. Gemma thinks that \u201cI want to move\u201d predicts \u201c my\u201d.\nThat\u2019s it!\nCongratulations, you\u2019ve done it \u2014 you\u2019ve made it to the end of Gemma. I hope it has been a fun ride (I have certainly enjoyed it!) There\u2019s not much of a conclusion, but if forced, I\u2019d offer:\n- Much of the transformer\u2019s operation comes down to the humble dot product.\n- As algorithms go, the transformer isn\u2019t all that complex.\nThat said, this was just a taster; here are a few out-of-scope topics I think are very interesting, in no particular order:\n- Training/fine-tuning.\n- Efficient implementation (especially parallel and distributed).\n- Low-precision computation (training and inference).\n- Integration into a full inference system, with batching, masking & search.\n- Evaluation methods.\n- Sparse computation.\n- Alternative architectures (e.g. RNN, S4).\n- Application to other domains.\n- Practical applications & alignment.\nThere\u2019s plenty to learn, then. I hope that we can make some progress together!\nThis article was originally published by Graphcore researcher Douglas Orr on his personal blog, Doug\u2019s Diversions in April 2024. It is reproduced here with his permission.",
    "annotations": [],
    "raw_file_hash": null,
    "asset_paths": [],
    "processing_status": "pending",
    "error_message": null,
    "obsidian_path": null,
    "tags": [],
    "metadata": {
      "raindrop_id": 899079516,
      "collection_id": 49346813,
      "cover": "https://graphcore-research.github.io/assets/images/posts/2024-04/gemma/attention.png",
      "excerpt": "Transformer-based LLMs seem mysterious, but they don\u2019t need to. In this post, we\u2019ll walk through a modern transformer LLM, Google\u2019s Gemma, providing bare-bones PyTorch code and some intuition for why each step is there. If you\u2019re a programmer and casual ML enthusiast, this is written for you."
    }
  },
  {
    "id": "71d6080e-b7a7-44e1-8280-9ba60f3a7ad9",
    "source_type": "article",
    "source_url": "https://pytorch.org/blog/flexattention/",
    "source_file_path": null,
    "title": "FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention",
    "authors": [
      "Unknown"
    ],
    "created_at": "2024-11-12T08:03:11.097000+00:00",
    "ingested_at": "2026-01-01T20:24:02.306025",
    "full_text": "In theory, Attention is All You Need. In practice, however, we also need optimized attention implementations like FlashAttention.\nAlthough these fused attention implementations have substantially improved performance and enabled long contexts, this efficiency has come with a loss of flexibility. You can no longer try out a new attention variant by writing a few PyTorch operators \u2013 you often need to write a new custom kernel! This operates as a sort of \u201csoftware lottery\u201d for ML researchers \u2013 if your attention variant doesn\u2019t fit into one of the existing optimized kernels, you\u2019re doomed to slow runtime and CUDA OOMs.\nFor some examples of attention variants, we have Causal, Relative Positional Embeddings, Alibi, Sliding Window Attention, PrefixLM, Document Masking/Sample Packing/Jagged Tensors, Tanh Soft-Capping, PagedAttention, etc. Even worse, folks often want combinations of these! Sliding Window Attention + Document Masking + Causal + Context Parallelism? Or what about PagedAttention + Sliding Window + Tanh Soft-Capping?\nThe left picture below represents the state of the world today \u2013 some combinations of masking + biases + setting have existing kernels implemented. But the various options lead to an exponential number of settings, and so overall we end up with fairly spotty support. Even worse, new attention variants researchers come up with will have zero support.\nTo solve this hypercube problem once and for all, we introduce FlexAttention, a new PyTorch API.\n- We provide a flexible API that allows implementing many attention variants (including all the ones mentioned in the blog post so far) in a few lines of idiomatic PyTorch code.\n- We lower this into a fused FlashAttention kernel through\ntorch.compile\n, generating a FlashAttention kernel that doesn\u2019t materialize any extra memory and has performance competitive with handwritten ones. - We also automatically generate the backwards pass, leveraging PyTorch\u2019s autograd machinery.\n- Finally, we can also take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations.\nWith FlexAttention, we hope that trying new attention variants will only be limited by your imagination.\nYou can find many FlexAttention examples at the Attention Gym: https://github.com/pytorch-labs/attention-gym. If you have any cool applications, feel free to submit an example!\nPS: We also find this API very exciting since it leverages a lot of existing PyTorch infra in a fun way \u2013 more on that in the end.\nFlexAttention\nHere is the classic attention equation:\nIn code form:\nQ, K, V: Tensor[batch_size, num_heads, sequence_length, head_dim]\nscore: Tensor[batch_size, num_heads, sequence_length, sequence_length] = (Q @ K) / sqrt(head_dim)\nprobabilities = softmax(score, dim=-1)\noutput: Tensor[batch_size, num_heads, sequence_length, head_dim] = probabilities @ V\nFlexAttention allows for an user-defined function score_mod:\nIn code form:\nQ, K, V: Tensor[batch_size, num_heads, sequence_length, head_dim]\nscore: Tensor[batch_size, num_heads, sequence_length, sequence_length] = (Q @ K) / sqrt(head_dim)\nmodified_scores: Tensor[batch_size, num_heads, sequence_length, sequence_length] = score_mod(score)\nprobabilities = softmax(modified_scores, dim=-1)\noutput: Tensor[batch_size, num_heads, sequence_length, head_dim] = probabilities @ V\nThis function allows you to modify the attention scores prior to softmax. Surprisingly, this ends up being sufficient for the vast majority of attention variants (examples below)!\nConcretely, the expected signature for score_mod\nis somewhat unique.\ndef score_mod(score: f32[], b: i32[], h: i32[], q_idx: i32[], kv_idx: i32[])\nreturn score # noop - standard attention\nIn other words, score\nis a scalar pytorch tensor that represents the dot product of a query token and a key token. The rest of the arguments tell you which dot product you\u2019re currently computing \u2013 b\n(current element in batch), h\n(current head), q_idx\n(position in query), kv_idx\n(position in key/value tensors).\nTo apply this function, we could implement it as\nfor b in range(batch_size):\nfor h in range(num_heads):\nfor q_idx in range(sequence_length):\nfor kv_idx in range(sequence_length):\nmodified_scores[b, h, q_idx, kv_idx] = score_mod(scores[b, h, q_idx, kv_idx], b, h, q_idx, kv_idx)\nOf course, this is not how FlexAttention is implemented under the hood. Leveraging torch.compile\n, we automatically lower your function into a single fused FlexAttention kernel \u2013 guaranteed or your money back!\nThis API ends up being surprisingly expressive. Let\u2019s look at some examples.\nScore Mod Examples\nFull Attention\nLet\u2019s first do \u201cfull attention\u201d, or standard bidirectional attention. In this case, score_mod\nis a no-op \u2013 it takes as input the scores and then returns them as is..\ndef noop(score, b, h, q_idx, kv_idx):\nreturn score\nAnd to use it end to end (including both forwards and backwards):\nfrom torch.nn.attention.flex_attention import flex_attention\nflex_attention(query, key, value, score_mod=noop).sum().backward()\nRelative Position Encodings\nOne common attention variant is the \u201crelative position encoding\u201d. Instead of encoding the absolute distance in the queries and keys, relative position encoding adjusts scores based on the \u201cdistance\u201d between the queries and keys.\ndef relative_positional(score, b, h, q_idx, kv_idx):\nreturn score + (q_idx - kv_idx)\nNote that unlike typical implementations, this does not need to materialize a SxS tensor. Instead, FlexAttention computes the bias values \u201con the fly\u201d within the kernel, leading to significant memory and performance improvements.\nALiBi Bias\nSource: Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\nALiBi was introduced in Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, and claims to have beneficial properties for length extrapolation at inference. Notably, MosaicML has pointed to \u201clack of kernel support\u201d as the main reason why they eventually switched from ALiBi to rotary embeddings.\nAlibi is similar to relative positional encodings with one exception \u2013 it has a per-head factor that is typically precomputed.\nalibi_bias = generate_alibi_bias() # [num_heads]\ndef alibi(score, b, h, q_idx, kv_idx):\nbias = alibi_bias[h] * (kv_idx - q_idx)\nreturn score + bias\nThis demonstrates one interesting piece of flexibility torch.compile\nprovides \u2013 we can load from alibi_bias\neven though it wasn\u2019t explicitly passed in as an input! The generated Triton kernel will calculate the correct loads from the alibi_bias\ntensor and fuse it. Note that you could regenerate alibi_bias\nand we still wouldn\u2019t need to recompile.\nSoft-capping\nSoft-capping is a technique used in Gemma2 and Grok-1 that prevents logits from growing excessively large. In FlexAttention, it looks like:\nsoftcap = 20\ndef soft_cap(score, b, h, q_idx, kv_idx):\nscore = score / softcap\nscore = torch.tanh(score)\nscore = score * softcap\nreturn score\nNote that we also automatically generate the backwards pass from the forwards pass here. Also, although this implementation is semantically correct, we likely want to use a tanh approximation in this case for performance reasons. See attention-gym for more details.\nCausal Mask\nAlthough bidirectional attention is the simplest, the original Attention is All You Need paper and the vast majority of LLMs use attention in a decoder-only setting where each token can only attend to the tokens prior to it. Folks often think of this as a lower-triangular mask, but with the score_mod\nAPI it can be expressed as:\ndef causal_mask(score, b, h, q_idx, kv_idx):\nreturn torch.where(q_idx >= kv_idx, score, -float(\"inf\"))\nBasically, if the query token is \u201cafter\u201d the key token, we keep the score. Otherwise, we mask it out by setting it to -inf, thus ensuring it won\u2019t participate in the softmax calculation.\nHowever, masking is special compared to other modifications \u2013 if something is masked out, we can completely skip its computation! In this case, a causal mask has about 50% sparsity, so not taking advantage of the sparsity would result in a 2x slowdown. Although this score_mod\nis sufficient to implement causal masking correctly, getting the performance benefits of sparsity requires another concept \u2013 mask_mod\n.\nMask Mods\nTo take advantage of sparsity from masking, we need to do some more work. Specifically, by passing a mask_mod\nto create_block_mask\n, we can create a BlockMask\n. FlexAttention can then use BlockMask\nto take advantage of the sparsity!\nThe signature of mask_mod\nis very similar to score_mod\n\u2013 just without the score\n. In particular\n# returns True if this position should participate in the computation\nmask_mod(b, h, q_idx, kv_idx) => bool\nNote that score_mod\nis strictly more expressive than mask_mod\n. However, for masking, it\u2019s recommended to use mask_mod\nand create_block_mask\n, as it\u2019s more performant. See the FAQ on why score_mod\nand mask_mod\nare separate.\nNow, let\u2019s take a look at how we might implement causal mask with mask_mod\n.\nCausal Mask\nfrom torch.nn.attention.flex_attention import create_block_mask\ndef causal(b, h, q_idx, kv_idx):\nreturn q_idx >= kv_idx\n# Because the sparsity pattern is independent of batch and heads, we'll set them to None (which broadcasts them)\nblock_mask = create_block_mask(causal, B=None, H=None, Q_LEN=1024, KV_LEN=1024)\n# In this case, we don't need a score_mod, so we won't pass any in.\n# However, score_mod can still be combined with block_mask if you need the additional flexibility.\nflex_attention(query, key, value, block_mask=block_mask)\nNote that create_block_mask\nis a relatively expensive operation! Although FlexAttention will not need to recompile when it changes, if you aren\u2019t careful about caching it, it can lead to significant slowdowns (check out the FAQ for suggestions on best practices).\nWhile the TFlops are roughly the same, the execution time is 2x faster for the mask_mod version! This demonstrates that we can leverage the sparsity that BlockMask provides us without losing hardware efficiency.\nSliding Window + Causal\nSource: Mistral 7B\nPopularized by Mistral, sliding window attention (also known as local attention) takes advantage of the intuition that the most recent tokens are the most useful. In particular, it allows the query token to only attend to, say, the 1024 most recent tokens. This is often used together with causal attention.\nSLIDING_WINDOW = 1024\ndef sliding_window_causal(b, h, q_idx, kv_idx):\ncausal_mask = q_idx >= kv_idx\nwindow_mask = q_idx - kv_idx <= SLIDING_WINDOW\nreturn causal_mask & window_mask\n# If you want to be cute...\nfrom torch.nn.attention import and_masks\ndef sliding_window(b, h, q_idx, kv_idx)\nreturn q_idx - kv_idx <= SLIDING_WINDOW\nsliding_window_causal = and_masks(causal_mask, sliding_window)\nWe benchmark it against F.scaled_dot_product_attention\nwith a sliding window mask as well as FA2 with a causal mask (as a reference point for performance). Not only are we significantly faster than F.scaled_dot_product_attention\n, we\u2019re also significantly faster than FA2 with a causal mask as this mask has significantly more sparsity.\nPrefixLM\nSource: PaliGemma: A versatile 3B VLM for transfer\nThe T5 architecture, proposed in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, describes an attention variant that performs full bidirectional attention on a \u201cprefix\u201d, and causal attention on the rest. We again compose two mask functions to accomplish this, one for causal masking and one that is based off of the prefix length.\nprefix_length: [B]\ndef prefix_mask(b, h, q_idx, kv_idx):\nreturn kv_idx <= prefix_length[b]\nprefix_lm_causal = or_masks(prefix_mask, causal_mask)\n# In this case, our mask is different per sequence so we set B equal to our batch size\nblock_mask = create_block_mask(prefix_lm_causal, B=B, H=None, S, S)\nJust like with score_mod\n, mask_mod\nallows us to refer to additional tensors that aren\u2019t explicitly an input to the function! However, with prefixLM, the sparsity pattern changes per input. This means that for each new input batch, we\u2019ll need to recompute the BlockMask\n. One common pattern is to call create_block_mask\nat the beginning of your model and reuse that block_mask\nfor all attention calls in your model. See Recomputing Block Masks vs. Recompilation.\nHowever, in exchange for that, we\u2019re not only able to have an efficient attention kernel for prefixLM, we\u2019re also able to take advantage of however much sparsity exists in the input! FlexAttention will dynamically adjust its performance based off of the BlockMask data, without needing to recompile the kernel.\nDocument Masking/Jagged Sequences\nAnother common attention variant is document masking/jagged sequences. Imagine that you have a number of sequences of varying length. You want to train on all of them together, but unfortunately, most operators only accept rectangular tensors.\nThrough BlockMask\n, we can support this efficiently in FlexAttention as well!\n- First, we flatten all sequences into a single sequence with sum(sequence lengths) tokens.\n- Then, we compute the document_id that each token belongs to.\n- Finally, in our\nmask_mod\n, we simply whether the query and kv token belong to the same document!\n# The document that each token belongs to.\n# e.g. [0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2] corresponds to sequence lengths 3, 2, and 6.\ndocument_id: [SEQ_LEN]\ndef document_masking(b, h, q_idx, kv_idx):\nreturn document_id[q_idx] == document_id[kv_idx]\nAnd that\u2019s it! In this case, we see that we end up with a blockdiagonal mask.\nOne interesting aspect about document masking is that it\u2019s easy to see how it might combine with an arbitrary combination of other masks . For example, we already defined prefixlm_mask\nin the previous section. Do we now need to define a prefixlm_document_mask\nfunction as well?\nIn these cases, one pattern we\u2019ve found quite useful is what we call a \u201chigher level modification\u201d. In this case, we can take an existing mask_mod\nand automatically transform it into one that works with jagged sequences!\ndef generate_doc_mask_mod(mask_mod, document_id):\n# Get unique document IDs and their counts\n_, counts = torch.unique_consecutive(document_id, return_counts=True)\n# Create cumulative counts (offsets)\noffsets = torch.cat([torch.tensor([0], device=document_id.device), counts.cumsum(0)[:-1]])\ndef doc_mask_wrapper(b, h, q_idx, kv_idx):\nsame_doc = document_id[q_idx] == document_id[kv_idx]\nq_logical = q_idx - offsets[document_id[q_idx]]\nkv_logical = kv_idx - offsets[document_id[kv_idx]]\ninner_mask = mask_mod(b, h, q_logical, kv_logical)\nreturn same_doc & inner_mask\nreturn doc_mask_wrapper\nFor example, given the prefix_lm_causal\nmask from above, we can transform it into one that works on on packed documents like so:\nprefix_length = torch.tensor(2, dtype=torch.int32, device=\"cuda\")\ndef prefix_mask(b, h, q_idx, kv_idx):\nreturn kv_idx < prefix_length\nprefix_lm_causal = or_masks(prefix_mask, causal_mask)\ndoc_prefix_lm_causal_mask = generate_doc_mask_mod(prefix_lm_causal, document_id)\nNow, this mask is \u201cblock-prefixLM-diagonal\u201d shaped. \ud83d\ude42\nThat\u2019s all of our examples! There are far more attention variants than we have space to list, so check out Attention Gym for more examples. We hope that the community will contribute some of their favorite applications of FlexAttention as well.\nFAQ\nQ: When does FlexAttention need to recompile?\nAs FlexAttention leverages torch.compile\nfor graph capture, it can actually avoid recompilation in a broad spectrum of cases. Notably, it does not need to recompile even if captured tensors change values!\nflex_attention = torch.compile(flex_attention)\ndef create_bias_mod(bias)\ndef bias_mod(score, b, h, q_idx, kv_idx):\nreturn score + bias\nreturn bias_mod\nbias_mod1 = create_bias_mod(torch.tensor(0))\nflex_attention(..., score_mod=bias_mod1) # Compiles the kernel here\nbias_mod2 = create_bias_mod(torch.tensor(2))\nflex_attention(..., score_mod=bias_mod2) # Doesn't need to recompile!\nEven changing the block-sparsity doesn\u2019t require a recompile. However, if the block-sparsity changes, we do need to recompute the BlockMask.\nQ: When should we recompute the BlockMask?\nWe need to recompute the BlockMask whenever the block-sparsity changes. Although computing the BlockMask is much cheaper than recompilation (on the order of hundreds of microseconds as opposed to seconds), you should still take care to not excessively recompute the BlockMask.\nHere are some common patterns and some recommendations on how you might approach them.\nMask never changes (e.g. causal mask)\nIn this case, you can simply precompute the block mask and cache it globally, reusing it for all attention calls.\nblock_mask = create_block_mask(causal_mask, 1, 1, S,S)\ncausal_attention = functools.partial(flex_attention, block_mask=block_mask)\nMask changes every batch (e.g. document masking)\nIn this case, we would suggest computing the BlockMask at the beginning of the model and threading it through the model \u2013 reusing the BlockMask for all layers.\ndef forward(self, x, doc_mask):\n# Compute block mask at beginning of forwards\nblock_mask = create_block_mask(doc_mask, None, None, S, S)\nx = self.layer1(x, block_mask)\nx = self.layer2(x, block_mask)\n...\n# amortize block mask construction cost across all layers\nx = self.layer3(x, block_mask)\nreturn x\nMask changes every layer (e.g. data-dependent sparsity)\nThis is the hardest setting, since we\u2019re unable to amortize the block mask computation across multiple FlexAttention invocations. Although FlexAttention can certainly still benefit this case, the actual benefits from BlockMask depend on how sparse your attention mask is and how fast we can construct the BlockMask. That leads us to\u2026\nQ: How can we compute BlockMask quicker?\ncreate_block_mask\nis unfortunately fairly expensive, both from a memory and compute perspective, as determining whether a block is completely sparse requires evaluating mask_mod\nat every single point in the block. There are a couple ways to address this:\n- If your mask is the same across batch size or heads, make sure that you\u2019re broadcasting over those (i.e. set them to\nNone\nincreate_block_mask\n). - Compile\ncreate_block_mask\n. Unfortunately, today,torch.compile\ndoes not work directly oncreate_block_mask\ndue to some unfortunate limitations. However, you can set_compile=True\n, which will significantly reduce the peak memory and runtime (often an order of magnitude in our testing). - Write a custom constructor for BlockMask. The metadata for BlockMask is quite simple (see the documentation). It\u2019s essentially two tensors. a.\nnum_blocks\n: The number of KV blocks computed for each query block.\nb.indices\n: The positions of the KV blocks computed for each query block.For example, here\u2019s a custom BlockMask constructor forcausal_mask\n.\ndef create_causal_mask(S):\nBLOCK_SIZE = 128\n# The first query block computes one block, the second query block computes 2 blocks, etc.\nnum_blocks = torch.arange(S // BLOCK_SIZE, device=\"cuda\") + 1\n# Since we're always computing from the left to the right,\n# we can use the indices [0, 1, 2, ...] for every query block.\nindices = torch.arange(S // BLOCK_SIZE, device=\"cuda\").expand(\nS // BLOCK_SIZE, S // BLOCK_SIZE\n)\nnum_blocks = num_blocks[None, None, :]\nindices = indices[None, None, :]\nreturn BlockMask(num_blocks, indices, BLOCK_SIZE=BLOCK_SIZE, mask_mod=causal_mask)\nQ: Why are score_mod\nand mask_mod\ndifferent? Isn\u2019t mask_mod\njust a special case of score_mod\n?\nVery astute question, hypothetical audience member! In fact, any mask_mod\ncan be easily converted to a score_mod\n(we do not recommend using this function in practice!)\ndef mask_mod_as_score_mod(b, h, q_idx, kv_idx):\nreturn torch.where(mask_mod(b, h, q_idx, kv_idx), score, -float(\"inf\"))\nSo, if score_mod\ncan implement everything mask_mod\ncan, what\u2019s the point of having mask_mod\n?\nOne immediate challenge: a score_mod\nrequires the actual score\nvalue as an input, but when we\u2019re precomputing the BlockMask, we don\u2019t have the actual score\nvalue. We can perhaps fake the values by passing in all zeros, and if the score_mod\nreturns -inf\n, then we consider it to be masked (in fact, we originally did this!).\nHowever, there are two issues. The first is that this is hacky \u2013 what if the user\u2019s score_mod\nreturned -inf\nwhen the input is 0? Or what if the user\u2019s score_mod\nmasked out with a large negative value instead of -inf\n? It seems we\u2019re trying to cram a round peg into a square hole. However, there\u2019s a more important reason to separate out mask_mod\nfrom score_mod\n\u2013 it\u2019s fundamentally more efficient!.\nAs it turns out, applying masking to every single computed element is actually quite expensive \u2013 our benchmarks see about a 15-20% degradation in performance! So, although we can get significant speedups by skipping half the computation, we lose a meaningful part of that speedup from needing to mask out every element!\nLuckily, if we visualize the causal mask, we notice that the vast majority of blocks do not require a \u201ccausal mask\u201d at all \u2013 they\u2019re fully computed! It is only the blocks on the diagonal, partially computed and partially masked, that require masking to be applied.\nThe BlockMask previously told us which blocks we need to compute and which blocks we can skip. Now, we further augment this data structure to also tell us which blocks are \u201cfully computed\u201d (i.e. masking can be skipped) vs. \u201cpartially computed\u201d (i.e. a mask needs to be applied). Note, however, that although masks can be skipped on \u201cfully computed\u201d blocks, other score_mod\ns like relative positional embeddings still need to be applied.\nGiven just a score_mod\n, there\u2019s no sound way for us to tell which parts of it are \u201cmasking\u201d. Hence, the user must separate these out themselves into mask_mod\n.\nQ: How much additional memory does the BlockMask need?\nThe BlockMask metadata is of size [BATCH_SIZE, NUM_HEADS, QUERY_LEN//BLOCK_SIZE, KV_LEN//BLOCK_SIZE].\nIf the mask is the same across the batch or heads dimension it can be broadcasted over that dimension to save memory.\nAt the default BLOCK_SIZE\nof 128, we expect that the memory usage will be fairly negligible for most use cases. For example, for a sequence length of 1 million, the BlockMask would only use 60MB of additional memory. If this is a problem, you can increase the block size: create_block_mask(..., BLOCK_SIZE=1024).\nFor example, increasing BLOCK_SIZE\nto 1024 would result in this metadata dropping to under a megabyte.\nQ: How do the numerics compare?\nAlthough the results are not bitwise identical, we are confident that FlexAttention is as numerically accurate as FlashAttention. We generate the following distribution of differences comparing FlashAttention versus FlexAttention over a large range of inputs on both causal and non causal attention variants. The errors are nearly identical.\nPerformance\nGenerally speaking, FlexAttention is nearly as performant as a handwritten Triton kernel, which is unsurprising, as we heavily leverage a handwritten Triton kernel. However, due to its generality, we do incur a small performance penalty. For example, we must incur some additional latency to determine which block to compute next. In some cases, we provide some kernel options that can affect the performance of the kernel while changing its behavior. They can be found here: performance knobs\nAs a case study, let\u2019s explore how the knobs affect the performance of causal attention. We will compare performance of the triton kernel versus FlashAttentionv2 on A100. The script can be found here.\nFlexAttention achieves 90% of FlashAttention2\u2019s performance in the forward pass and 85% in the backward pass. FlexAttention is currently utilizing a deterministic algorithm that recomputes more intermediates than FAv2, but we have plans to improve FlexAttention\u2019s backward algorithm and hope to close this gap!\nConclusion\nWe hope you have as much fun using FlexAttention as we did developing it! While working on this, we ended up finding way more applications of this API than we could have expected. We\u2019ve already seen it accelerate torchtune\u2019s sample packing throughput by 71%, replace the need for a researcher to spend over a week writing their own custom Triton kernel, and deliver competitive performance with custom handwritten attention variants.\nOne final thing that made implementing FlexAttention quite fun is that we were able to leverage a lot of existing PyTorch infra in an interesting way. For example, one of the unique aspects about TorchDynamo (torch.compile\u2019s frontend) is that it does not require tensors used in the compiled function to be explicitly passed in as inputs. This allows us to compile mods like document masking, which require accessing global variables where the global variables need to change!\nbias = torch.randn(1024, 1024)\ndef score_mod(score, b, h, q_idx, kv_idx):\nreturn score + bias[q_idx][kv_idx] # The bias tensor can change!\nFurthermore, the fact that torch.compile\nis a generic graph-capture mechanism also allows it to support more \u201cadvanced\u201d transformations, such as the higher order transform that transforms any mask_mod\ninto one that works with jagged tensors.\nWe also leverage TorchInductor (torch.compile\u2019s backend) infrastructure for Triton templates. Not only did this make it easy to support codegening FlexAttention \u2013 it also automatically gave us support for dynamic shapes as well as epilogue fusion (i.e. fusing an operator onto the end of attention)! In the future, we plan on extending this support to allow for quantized versions of attention or things like RadixAttention as well.\nIn addition, we also leveraged higher order ops, PyTorch\u2019s autograd to automatically generate the backwards pass, as well as vmap to automatically apply score_mod\nfor creating the BlockMask.\nAnd, of course, this project wouldn\u2019t have been possible without Triton and TorchInductor\u2019s ability to generate Triton code.\nWe look forward to leveraging the approach we used here to more applications in the future!\nLimitations and Future Work\n- FlexAttention is currently available in PyTorch nightly releases, we plan to release it as a prototype feature in 2.5.0\n- We did not cover how to use FlexAttention for inference here (or how to implement PagedAttention) \u2013 we will cover those in a later post.\n- We are working to improve the performance of FlexAttention to match FlashAttention3 on H100 GPUs.\n- FlexAttention requires that all sequence lengths be a multiple of 128 \u2013 this will be addressed soon.\n- We plan on adding GQA support soon \u2013 for now, you can just replicate the kv heads.\nAcknowledgements\nWe want to highlight some prior work (and people) that have inspired FlexAttention.\n- Tri Dao\u2019s work on FlashAttention\n- Francisco Massa and the Xformers team for BlockSparseAttention in Triton\n- The Jax team\u2019s work on SplashAttention\n- Philippe Tillet and Keren Zhou for helping us with Triton\n- Ali Hassani for discussions on neighborhood attention\n- Everybody who\u2019s complained about attention kernels not supporting their favorite attention variant \ud83d\ude42",
    "annotations": [],
    "raw_file_hash": null,
    "asset_paths": [],
    "processing_status": "pending",
    "error_message": null,
    "obsidian_path": null,
    "tags": [],
    "metadata": {
      "raindrop_id": 898614957,
      "collection_id": 49346813,
      "cover": "https://pytorch.org/assets/images/social-share.jpg",
      "excerpt": ""
    }
  },
  {
    "id": "7d446365-4818-4c17-923a-f5200bf92c87",
    "source_type": "article",
    "source_url": "https://transformer-circuits.pub/2024/crosscoders/index.html?utm_source=tldrai",
    "source_file_path": null,
    "title": "Sparse Crosscoders for Cross-Layer Features and Model Diffing",
    "authors": [
      "Unknown"
    ],
    "created_at": "2024-11-05T07:08:13.510000+00:00",
    "ingested_at": "2026-01-01T20:24:02.599586",
    "full_text": "This note introduces sparse crosscoders, a variant of sparse autoencoders (e.g.\nThis note will cover some theoretical examples motivating crosscoders, and then present preliminary experiments applying them to cross-layer superposition and model diffing. We also briefly discuss the theory of how crosscoders might simplify circuit analysis, but leave results on this for a future update.\nAccording to the superposition hypothesis, neural networks represent more features than they have neurons by allowing features to be non-orthogonal\nAt first blush, the idea that this kind of superposition might be spread across layers might seem strange. But if we think about it carefully, it's actually relatively natural in the context of a transformer with a reasonable number of layers.\nOne interesting property of transformers is that, because the residual stream is linear, we can draw them as different, equivalent graphs. The following graph highlights the idea that two layers can be thought of as \"almost parallel branches\", except that they have an extra edge that allows the earlier layer to influence the later.\nIf we consider a one-step circuit computing a feature, we can imagine implementations where the circuit is split across two layers, but functionally is in parallel. This might actually be quite natural if the model has more layers than the length of the circuit it is trying to compute!\nIf features are jointly represented by multiple layers, where some of their activity can be understood as being in parallel, it's natural to apply dictionary learning to them jointly. We call this setup a crosscoder, and will return to it in the next section.\nIt's worth noting that jointly applying dictionary learning to multiple vectors is precisely what we do when models literally have parallel branches with cross-branch superposition\nCrosscoders can help us when there's cross-layer superposition, but they can also help us when a computed feature stays in the residual stream for many layers. Consider the following hypothetical \"feature lifecycle\" through the residual stream:\nIf we tried to understand this in terms of a residual stream feature at every layer, we'd have lots of duplicate features across layers. This can lead to circuits which seem much more complex than they need to.\nConsider the following hypothetical example, in which features 1 and 2 are present by layer L, and are combined (say via an \"and\") to form feature 3 via MLPs in layers L+2 and L+3, and then all three features persist in layer L+4. On the left panel of the figure below, we see that per-layer SAEs would produce 13 features in total, corresponding to features 1, 2, and 3 at each layer they are present. The causal graph relating them has many arrows, most for persistence (a feature causes itself in later layers) and two for each of the stages in which feature 3 is computed from 1 and 2. An ideal crosscoder picture, on the right, would have just three features and a simple causal graph.\nThis means that crosscoders may also give us a strategy for radically simplifying circuits if we use an appropriate architecture where, as in the above picture, feature encoders read in from a single residual stream layer and their decoders write out to downstream layers.\nAs an example suppose we have feature\nWe note, however, that there are some conceptual risks with this approach \u2013 the causal description it provides likely differs from that of the underlying model. We plan to explore this approach further in future updates.\nWhere autoencoders encode and predict activations at a single layer, and transcoders\nWe can think of autoencoders and transcoders as special cases of the general family of crosscoders.\nThe basic setup of a crosscoder is as follows. First, we compute the vector of feature activations\nwhere\nAnd have a loss:\nNote that the regularization term can be rewritten as:\nThat is, we weight the L1 regularization penalty by the L1 norm of the per-layer decoder weight norms (\nHowever, there are a two reasons to prefer the L1 norm version:\nOn the other hand, the L2 version more efficiently optimizes the frontier of MSE and global L0 across all layers of the model. Thus, for applications where uncovering layer or model-specific features is not important, and where it is not important to be able to compare loss values to per-layer SAEs, the L2-of-norms version may be preferable. In this report, all experiments used the L1-of-norms version.\nThis basic version above is what we'd call an \"acausal crosscoder\". Many variants are in fact possible. In particular, several important dimensions are:\nThe following table summarizes the variants:\nWe have found both weakly and strictly causal crosscoders helpful for simplifying feature interaction graphs in our circuits work, but there remain open questions as to how faithfully validate these analyses. Note that strictly causal crosscoder layers as presented here cannot capture the computation performed by attention layers. Some possibilities we are exploring include: (1) using strictly causal crosscoders to capture MLP computation and treating the computation performed by attention layers as linear (by conditioning on the empirical attention pattern for a given prompt), (2) combining strictly causal crosscoders for MLP outputs with weakly causal crosscoders for attention outputs, (3) developing interpretable attention replacement layers that could be used in combination with strictly causal crosscoders to form a \u201creplacement model.\u201d\nCan crosscoders actually uncover cross-layer structure? To explore this question, we first trained a global, acausal crosscoder on the residual stream activations of all layers of an 18-layer model. We compared its performance to that of 18 SAEs trained separately on each of the residual stream layers. We used a fixed L1 coefficient for the sparsity penalty. Note that we designed our loss to be comparable to a baseline SAE loss with the same L1 penalty, as discussed above. We separately normalize the activations of each layer prior to training the crosscoder, so that each layer contributes comparably to the loss.\nFor each approach, we swept over the number of training steps and the number of total features to select the optimal number of features at different FLOPS budgets. We are interested in how the dictionary performance scales with the total number of features in the crosscoder / across all SAEs, and with the amount of compute used in training. Note that for a model with L layers, a global, acausal crosscoder with F total features uses the same number of training FLOPS as a collection of per-layer SAEs with F features each (and thus with L*F total features). Or viewed another way, a collection of single-layer SAEs with F total dictionary features summed across all the SAEs can be trained with L times fewer FLOPS than a single crosscoder with F dictionary features. Thus, crosscoders must substantially outperform SAE on a \u201cper-feature efficiency\u201d basis to be competitive in terms of FLOPs.\nFirst we measure the eval loss of both approaches (MSE + decoder norm-weighted L1 norm, summed across layers):\nWe found that, controlling for the total number of features across layers, crosscoders substantially outperform per-layer SAEs on eval loss. This result indicates that there is a significant degree of redundant (linearly correlated) structure across layers, which are interpreted by the crosscoder as cross-layer features. However, with respect to training FLOPS, crosscoders are less efficient than per-layer SAEs at achieving the same eval loss, by a factor of about 2 at large compute budgets.\nIn other words, for a fixed number of total features, crosscoders are able to make more efficient use of their resources by identifying shared structure across layers, allowing them to lump together identical features across layers as a single cross-layer feature, which frees up the crosscoder\u2019s resources to spend on other features. However, identifying this structure costs compute at training time.\nHowever, eval loss is only one measure of the crosscoder\u2019s usefulness. Since our loss scales the sparsity penalty by the sum of decoder norms across layers, it effectively measures (an L1 relaxation of) the sparsity of (feature, layer) tuples. Thus, it provides a sense of how well any single layer of the model can be described as a sparse sum of crosscoder features, vs. SAE features. However, we may also be interested in how well the activity across the entire model can be described as a sparse sum of crosscoder features, vs. SAE features. For this purpose, the metric of interest is the (MSE, L0) value of each method, where in the per-layer SAE case we sum L0 norm across all the SAEs. We show (MSE, L0) values at optimal values of SAE / crosscoder training loss over a set of values of training FLOPS.\nViewed from this perspective, crosscoders provide a dramatic benefit over per-layer SAEs. By consolidating shared structure across layers, they exhibit a much less redundant (and therefore more concise) decomposition of the entire model\u2019s activations. In theory, the same consolidation might be achievable via post-hoc analysis on SAE features, e.g. by clustering features based on the similarity of their activations. However, in practice, this analysis may be difficult, particularly due to stochasticity in SAE training. Crosscoders effectively \u201cbake in\u201d this clustering at training time.\nSummarizing the results at a high level, the efficiency crosscoders and per-layer SAEs can be compared in essentially two ways. In terms of the tradeoff between reconstruction error and the sparsity of the feature set used to reconstruct a single layer\u2019s activity, crosscoders make more efficient use of dictionary features but less efficient use of training FLOPS. In terms of the reconstruction error / sparsity tradeoff in reconstructing the entire model\u2019s activity, crosscoders provide an unambiguous advantage, by resolving redundant structure across layers.\nWe next conducted some basic analyses of the crosscoder features. We were especially interested in features\u2019 behavior across layers: (1) Do crosscoder features tend to be localized to a few layers, or do they span the whole model? (2) Do crosscoder features\u2019 decoder vector directions remain stable across layers, or can the same feature point in different directions in different layers?\nAddressing question (1), below we plot the decoder weight norms of 50 randomly sampled crosscoder features across the layers of the model (which are representative of trends we have observed in the full collection of features). For each feature, we rescale the norms so that the maximum value is 1, for ease of visual comparison.\nWe see that most features tend to peak in strength in a particular layer, and decay in earlier and later layers. Sometimes the decay is sudden, indicating a localized feature, but often it is more gradual, with many features having substantial norm across most or even all layers.\nThe ability to track the presence of features across layers is spiritually similar to results by Yun et al.\nReturning to the above plot, is the existence of gradual formation of features distributed across layers evidence for cross-layer superposition? While it's definitely consistent with the hypothesis, it could also have other explanations. For example, a feature could be unambiguously produced at one layer and then amplified at the next layer. More research \u2013 ideally circuit analysis \u2013 would be needed to confidently interpret the meaning of gradual feature formation.\nWe now return to the second of our original questions, regarding the embedding directions of crosscoder features. Below, for a few example crosscoder features, we show:\nThe leftmost column is an example of a feature whose decoder direction drifts across layers at roughly the same spatial scale at which its norm decays. The middle column is an example of a feature whose decoder direction is fairly stable over the layers in which the feature has appreciable norm. The right column is an example of a feature that persists throughout the model, but with rapidly changing decoder direction.\nThese examples were selected to illustrate the range of feature archetypes we find. Below, we show this information for 36 randomly selected features to give a more representative picture.\nOverall, we find that most features\u2019 decoder directions are much more stable across layers than would be expected by chance, but also that they drift substantially across layers, even in layers where the feature decoder norm remains strong. The specific behavior varies considerably by feature. This suggests that the cross-layer features uncovered by our crosscoders are not simply passively relayed via residual connections.\nNote that we do not conduct a systematic analysis of qualitative feature interpretability in this work. Anecdotally, we find that crosscoder features are similarly interpretable to sparse autoencoder features, and crosscoder features that peak in a particular layer are qualitatively similar to features obtained from a sparse autoencoder trained on that layer. We plan to more rigorously evaluate crosscoder feature interpretability in future work.\nWe experimented with locally masked \u201cconvolutional\u201d variants of crosscoders, in which each feature is assigned a local window of K layers that it is responsible for encoding / decoding. We hoped that this would allow us to capture the benefits of crosscoders while minimizing the FLOPS expense at crosscoder training time. However, we found that eval loss interpolated fairly linearly as we varied the convolutional window K from 1 (per-layer SAE case) to n_layers (global acausal crosscoder) \u2013 there was no obvious inflection point that optimized the performance / cost tradeoff. Put another way, the performance of a locally masked cross-coder was similar to that of a smaller, FLOPS-matched global crosscoder. This is consistent with the picture from the distribution of features across layers, as seen in the previous section.\nWe also experimented with \u201cweakly causal\u201d crosscoders. We focused in particular on an architecture in which each feature is assigned an encoder layer i \u2013 its encoder reads in from layer i alone, and its decoder attempts to reconstruct layer i and all subsequent layers. We found that in terms of eval loss performance, this architecture\u2019s FLOPS efficiency is in between that of per-layer SAEs (slightly worse) and global, acausal crosscoders (slightly better). With respect to dictionary size, its performance lagged behind that of global crosscoders by a factor of 3 to 4.\nWe have also conducted preliminary experiments with strictly causal \u201ccross-layer transcoders,\u201d in which each feature reads in from the residual stream at a layer L, and attempts to predict the output of the MLPs in layers L, L+1, L+2, \u2026 NUM_LAYERS. When examining the decoder norms of these features, we find a mix of:\nOne interesting application of crosscoders is to analyze the differences in feature sets before and after an MLP layer, and the computations that give rise to \u201cnew\u201d features in the MLP output (see the section on Model Diffing for related experiments analyzing cross-model differences). To achieve this, we can train a crosscoder on the pre-MLP residual stream space and the outputs that the MLP writes back to the residual stream. We use a masking strategy in which the features\u2019 encoders read only from the pre-MLP space, but their decoders attempt to reconstruct both the pre-MLP activity and the MLP output. Note that this architecture differs from a regular transcoder, in which the features are only responsible for reconstructing the MLP output.\nThis architecture has two nice properties. First, it allows us to identify features that are shared between the pre and post-MLP spaces, and features that are specific to one or the other. To see this, we can plot the relative norms of the decoder vectors within each space. Remarkably, we see a clear trimodal structure, corresponding to pre-only, shared, and post-only (i.e. \u201cnewly computed\u201d) features.\nSecond, because we constrained the encoder vectors to live only in the pre-MLP space, this architecture allows us to analyze how these \u201cnewly computed\u201d features were computed from existing features in the residual stream (similar to how one would analyze a transcoder). In particular, the inputs to a newly created feature can be computed by taking the dot product of the downstream feature\u2019s encoder vector with the upstream features\u2019 decoder vectors, and weighting by the source features activation (either in a particular context, or averaged over dataset examples). Anecdotally, we often find that the post-MLP feature represents a more abstract concept and its strongest inputs are specific instances of that concept. For instance, we find one post-MLP feature that activates on words indicating uniqueness, like \u201cspecial,\u201d \u201cparticular,\u201d \u201cexceptional,\u201d etc., and its pre-MLP inputs each fire for particular words in this category, in particular contexts.\nWe also analyzed the extent to which \u201cstable\u201d features (arbitrarily those with between 0.3 and 0.7 relative decoder norm weight in post-MLP space) tend to be embedded along similar directions in pre- and post-MLP space. Interestingly, we found a positive correlation on average, but with high variance, and relatively low in absolute terms. This result may explain why feature directions tend to drift across layers \u2013 it appears that MLP layers relay many features without nonlinear modification, but along a different axis .\nWe introduced crosscoders as a way to understand cross-layer features, but the same approach can be used to extract cross-model features. In this section, we'll study the use of cross-model features to compare and \"diff\" models. Our results here are very preliminary, and while there are significant signs of life, we also find that this strategy produces many features we don't understand.\nThere's a long history of researchers seeking to compare neural networks. Of course, comparing the functional differences of different neural networks \u2013 for example, measuring their performance on benchmarks \u2013 is a central part of the machine learning paradigm. But it's natural to ask deeper questions. How do their representations compare? How do they compare mechanistically? A host of methods have developed attempting to address these questions. We can divide them into a few categories:\nEntire Representations. A significant body of work studies how similar two neural network representations are, often producing aggregate measures of representation similarity. The earliest attack on this problem we're aware of is a philosophical paper by Laakso & Cottrell\nNeurons and Features. If our goal is interpretability, we likely want a finer grained way to reason about the similarity of neural networks. We want to know if neurons or features are similar, even if the networks as a whole are not. We also wish to know what those similar or dissimilar features actually are, and to what extent there may be \"universal\" features across models. Early work by Li et al.\nOther Interpretability Objects. Although the existence of universal features and circuits is the most investigated topic in this space, it's worth noting the existence of preliminary evidence that other \"interpretability objects\" may be universal. Once analogous features are discovered between models, it may be possible to identify analogous circuits. Schubert et al.\nModel Diffing. As the idea of universal features and circuits became more widespread, interest began to arise in the idea of \"diffing\" models as a way to make safety auditing easier. Just as we review software in terms of incremental diffs, you might hope to review the safety of models by focusing on how it has changed from a previously deployed model. To the best of our knowledge, this \"model diffing\" problem was originally articulated by the OpenAI Clarity Team in 2018.\nComparison of Finetuned Models. An important application of model diffing is comparing multiple finetuned versions of one model, or comparing a finetuned model to the original version before finetuning. This is both of immediate applied interest (finetuning is used extensively for commercially deployed models and it would be useful to compare different finetuning strategies) and of longer-term safety interest (many theoretical arguments for safety risk suggest that finetuned models are more likely to be dangerous, especially if they're finetuned with RL).\nSeveral recent results suggest that finetuned models use similar mechanism as the base model from which they were trained\nOne of the exciting things about crosscoders is they're not limited to closely related models (such as a finetuned version of a particular base model). Instead, we can get cross-model features between any models, including across:\nThis isn't limited to two models. Subject to computational constraints, we can get cross-model features between arbitrary numbers of models. Of course, when models are significantly different, we may not know what the analogous layers to compare between the models are; in this case, we may wish to also have our cross-coder extend across layers.\nOnce we get cross-model features, we can study:\nFor this preliminary writeup, we will limit ourselves to two experiments. First, we will study how finetuning affected Claude 3.0 Sonnet, diffing the middle layer of the base model against its finetuned counterpart. We'll then turn our attention to scaling, and study how features and their distribution over layers varies as a function of scale. These are very preliminary experiments, and we'll only provide initial analyses, leaving more detailed investigations to potential future work.\nWe trained a crosscoder with 1 million features on the residual stream activations from the middle layer of Claude 3 Sonnet and the base model from which it was finetuned. We wanted to test whether the crosscoder could decompose the models\u2019 activations into shared features and model-specific features. These model-specific features would indicate features learned, or forgotten, during finetuning.\nTo test this, we looked at the relative norms of feature decoder weights in the two models. Remarkably, we found that features cluster into three obvious groups \u2013 base model-specific features, finetuned model-specific features, and shared features. In this example, there are between four and five thousand model-specific features for each model, out of a total 1 million features.\nWe found a few particular examples of finetuned model-specific features particularly notable.\nWe also noticed some interesting base model-specific features, which represent Human/Assistant interactions that are at odds with the kinds of interactions Claude is trained to have:\nThese features are cherrypicked. Unfortunately, we've also found that the majority of the model-exclusive features are not immediately interpretable. However, inspecting the features that activate on tokens of interest often reveals clearly interpretable features, leading to a mixed picture we're still working to understand.\nFor the shared features, we checked whether their decoders vectors are aligned in the two models. In almost all cases, they were highly aligned, suggesting that these features do in fact represent the same concept, and perform the same function, in the two models. However, we also found that for a few thousand features, the correlation was very low or even negative. We have not investigated this phenomenon in depth, but we suspect that these indicate cases where the finetuned model uses a concept that was present in the base model, but in a new way.\nNote that recent work by Kissane et al.\nWe trained an acausal crosscoder on ten evenly spaced layers from three models of increasing sizes. We were interested in how much shared structure exists across different models, and which layers correspond to which across models. Our analysis is in very preliminary stages; however, we have obtained one interesting result.\nFor each feature, we measured the norm of its decoder in each (layer, model) pair (producing a 3 models x 10 layers = 30-dimensional vector for each feature). We then applied nonnegative matrix factorization to these norm vectors across all features. The NMF components provide a window into which (layer, model) pairs tend to share features, and also which features contribute to this shared structure.\nThe example below shows the results of running NMF with four components, assigning a different color to each component (left), and the spectrum of feature loadings onto each component (right). Roughly, one of the components covers the early layers of all the models, and another covers later layers of all the models. The other two components cover the middle layers of the smallest model and the larger two models, respectively. This suggests that qualitatively new representations emerge in middle model layers as model scale increases. We are interested in qualitatively exploring the features responsible for these differences in future work.\nOne of the exciting things about crosscoders (and especially the model diffing component) is that they may give us a fresh line of attack on many very basic questions. To give just a few examples:\nIt would be very exciting to see some of these investigated. However, this does rely heavily on crosscoder-based model diffing, and as discussed above, our results there are a bit mixed.\nOne might see the transition from interpreting neurons, to SAE or transcoder features, to crosscoders as progressively moving away from the literal, surface-level model that we're studying. Each step of abstraction allows us to escape unfortunate details of how circuits may be scaffolded onto models, but at the cost of moving further away from the ground truth.\nIf we set things up carefully \u2013 in particular, using \"error features\" (see eg.\nDespite these downsides, it seems quite plausible that the interpretability advantages of studying simplified isomorphisms to the underlying model will be significant enough to favor this approach.\nWe are deeply grateful to our colleagues Adam Jermyn, Brian Chen, Craig Citro, Emmanuel Ameisen, Thomas Henighan, Kelley Rivoire, Nick Turner, Adam Pearce, Rodrigo Luger, Shan Carter, Siddharth Mishra-Sharma, Hoagy Cunningham, Andrew Persic, Callum McDougall, Trenton Bricken, and Wes Gurnee. Thanks as well to Martin Wattenberg, Daniel Murfet, Neel Nanda, Liv Gorton, Gabriel Goh, and Nick Cammarata for helpful remarks.",
    "annotations": [],
    "raw_file_hash": null,
    "asset_paths": [],
    "processing_status": "pending",
    "error_message": null,
    "obsidian_path": null,
    "tags": [],
    "metadata": {
      "raindrop_id": 893223780,
      "collection_id": 49346813,
      "cover": "https://rdl.ink/render/https%3A%2F%2Ftransformer-circuits.pub%2F2024%2Fcrosscoders%2Findex.html%3Futm_source%3Dtldrai",
      "excerpt": ""
    }
  }
]